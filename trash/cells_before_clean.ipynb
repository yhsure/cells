{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":612,"status":"ok","timestamp":1675951322490,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"oSbu8jTsZh8s","outputId":"2b5f9fcd-c770-48b2-c717-cca3995b99b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/cells\n"]}],"source":["cd drive/MyDrive/Colab\\ Notebooks/cells"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265940,"status":"ok","timestamp":1675953961064,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"NC2-tZ5QZsmw","outputId":"e1c16d1c-16cb-4bfd-cc78-fd9f7bb0d610"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([86024, 2766])\n"]}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.io import mmread\n","\n","\n","class RNA(Dataset):\n","    def __init__(self, data_file):\n","        # load the scRNA-seq data from the specified file\n","        self.data = torch.from_numpy(\n","            mmread(data_file).astype(\"float32\").transpose().todense())\n","        print(self.data.shape)\n","        \n","    def __len__(self):\n","        # return the number of examples in the dataset\n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        # return the preprocessed data for the specified example\n","        library = self.data[index].sum(dim=-1)\n","        example = self.data[index]\n","        return example, library\n","\n","        \n","\n","# datasets: hcl, celegan, uc_epi, zfish_ep50_5226\n","# train_dataset = RNA(\"cells/data/hcl.mtx\")\n","# train_dataset = RNA(\"data/zfish_ep50_5226.mtx\")\n","train_dataset = RNA(\"data/celegan.mtx\")\n","train_loader = DataLoader(train_dataset, batch_size=2**12, shuffle=True) # 2**15\n"]},{"cell_type":"code","execution_count":190,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1675951332641,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"oeKzvTc0Z1Il","outputId":"240d613c-7353-4915-ed91-8312a2e48cee"},"outputs":[{"data":{"text/plain":["VAE(\n","  (encoder): Encoder(\n","    (fc0): Linear(in_features=2766, out_features=1383, bias=True)\n","    (fc1): Linear(in_features=1383, out_features=691, bias=True)\n","    (fc2): Linear(in_features=691, out_features=345, bias=True)\n","    (fc3_mean): Linear(in_features=345, out_features=2, bias=True)\n","    (fc3_var): Linear(in_features=345, out_features=2, bias=True)\n","  )\n","  (decoder): Decoder(\n","    (fc1): Linear(in_features=2, out_features=345, bias=True)\n","    (fc2): Linear(in_features=345, out_features=691, bias=True)\n","    (fc3): Linear(in_features=691, out_features=1383, bias=True)\n","    (fc4): Linear(in_features=1383, out_features=2766, bias=True)\n","  )\n",")"]},"execution_count":190,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch import optim\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, d, latent_dim):\n","        super(Encoder, self).__init__()\n","        self.fc0 = nn.Linear(in_features=d, out_features=d//2)\n","        self.fc1 = nn.Linear(in_features=d//2, out_features=d//4)\n","        self.fc2 = nn.Linear(in_features=d//4, out_features=d//8)\n","        self.fc3_mean = nn.Linear(in_features=d//8, out_features=latent_dim)\n","        self.fc3_var  = nn.Linear(in_features=d//8, out_features=latent_dim)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc0(x))\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        mu = self.fc3_mean(x)\n","        logvar = self.fc3_var(x)\n","        return mu, logvar\n","    \n","class Decoder(nn.Module):\n","    def __init__(self, d, latent_dim):\n","        super(Decoder, self).__init__()\n","        self.fc1 = nn.Linear(in_features=latent_dim, out_features=d//8)\n","        self.fc2 = nn.Linear(in_features=d//8, out_features=d//4)\n","        self.fc3 = nn.Linear(in_features=d//4, out_features=d//2)\n","        self.fc4 = nn.Linear(in_features=d//2, out_features=d)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = self.fc4(x)\n","        return x\n","\n","\n","class VAE(nn.Module):\n","    def __init__(self, d=2766, latent_dim=2, kl_weight=1):\n","        super(VAE, self).__init__()\n","        self.encoder = Encoder(d, latent_dim)\n","        self.decoder = Decoder(d, latent_dim)\n","        self.kl_weight = kl_weight\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return eps * std + mu\n","\n","    def forward(self, x, library=None):\n","        mu, logvar = self.encoder(x)\n","        z = self.reparameterize(mu, logvar)\n","        recon = self.decoder(z)\n","\n","        if library is not None:\n","            recon = F.softmax(recon, dim=-1)\n","            recon = recon * library.unsqueeze(-1)\n","            \n","        return recon, mu, logvar\n","\n","    def loss(self, x, recon, mu, logvar):\n","        mse = F.mse_loss(recon, x)\n","        kld = torch.mean(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n","        return mse, kld\n","\n","\n","dev = \"cpu\"    \n","model = VAE(d=next(iter(train_loader))[0].shape[-1], latent_dim=2).to(dev)\n","model.train()\n","opt = optim.Adam(model.parameters())"]},{"cell_type":"code","execution_count":336,"metadata":{},"outputs":[],"source":["# save model\n","# torch.save(model.state_dict(), \"models/VAE.pt\")\n","# model.load_state_dict(torch.load(\"VAE.pt\"))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["GMVAE(\n","  (encStack): Sequential(\n","    (0): Linear(in_features=2766, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=256, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=256, out_features=256, bias=True)\n","    (5): ReLU()\n","  )\n","  (fc_mu_z): Linear(in_features=256, out_features=2, bias=True)\n","  (fc_logvar_z): Linear(in_features=256, out_features=2, bias=True)\n","  (fc_mu_px): ModuleList(\n","    (0): Linear(in_features=256, out_features=2, bias=True)\n","    (1): Linear(in_features=256, out_features=2, bias=True)\n","    (2): Linear(in_features=256, out_features=2, bias=True)\n","    (3): Linear(in_features=256, out_features=2, bias=True)\n","    (4): Linear(in_features=256, out_features=2, bias=True)\n","  )\n","  (fc_logvar_px): ModuleList(\n","    (0): Linear(in_features=256, out_features=2, bias=True)\n","    (1): Linear(in_features=256, out_features=2, bias=True)\n","    (2): Linear(in_features=256, out_features=2, bias=True)\n","    (3): Linear(in_features=256, out_features=2, bias=True)\n","    (4): Linear(in_features=256, out_features=2, bias=True)\n","  )\n","  (decStack): Sequential(\n","    (0): Linear(in_features=2, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=256, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=256, out_features=2766, bias=True)\n","  )\n",")"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.distributions as D\n","\n","class GMVAE(nn.Module):\n","    def __init__(self, in_size, x_size, hidden_size, K):\n","        super(GMVAE, self).__init__()\n","        self.in_size = in_size\n","        self.x_size = x_size\n","        self.hidden_size = hidden_size\n","        self.K = K\n","        \n","        # Encoder\n","        self.encStack = nn.Sequential(\n","            nn.Linear(self.in_size, self.hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(self.hidden_size, self.hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(self.hidden_size, self.hidden_size),\n","            nn.ReLU())\n","        \n","        self.fc_mu_z = nn.Linear(self.hidden_size, self.x_size)\n","        self.fc_logvar_z = nn.Linear(self.hidden_size, self.x_size)\n","        self.fc_mu_px = nn.ModuleList(\n","            [nn.Linear(self.hidden_size, self.x_size) for i in range(self.K)])\n","        self.fc_logvar_px = nn.ModuleList(\n","            [nn.Linear(self.hidden_size, self.x_size) for i in range(self.K)])\n","        \n","        # Decoder\n","        self.decStack = nn.Sequential(\n","            nn.Linear(self.x_size, self.hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(self.hidden_size, self.hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(self.hidden_size, self.in_size))\n","        \n","    def encode(self, x):\n","        h = self.encStack(x)\n","        mu_z = self.fc_mu_z(h)\n","        logvar_z = self.fc_logvar_z(h)\n","        z = self.reparameterize(mu_z, logvar_z)\n","        return mu_z, logvar_z, z\n","    \n","    def decode(self, z):\n","        h = self.decStack(z)\n","        mu_px = [fc(h) for fc in self.fc_mu_px]\n","        logvar_px = [fc(h) for fc in self.fc_logvar_px]\n","        return mu_px, logvar_px\n","    \n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","    \n","    def forward(self, x):\n","        mu_z, logvar_z, z = self.encode(x)\n","        mu_px, logvar_px = self.decode(z)\n","        p = [D.Normal(mu, torch.exp(0.5*logvar)) for mu, logvar in zip(mu_px, logvar_px)]\n","        p = [dist.rsample() for dist in p]\n","        return mu_z, logvar_z, mu_px, logvar_px, p\n","\n","    def loss(self, x):\n","        mu_z, logvar_z, mu_px, logvar_px, p = self.forward(x)\n","        kl = self.kl_div(mu_z, logvar_z)\n","        kl = kl.sum(dim=1).mean()\n","        nll = self.neg_log_likelihood(x, mu_px, logvar_px, p)\n","        nll = nll.sum(dim=1).mean()\n","        loss = kl + nll\n","        return loss, kl, nll\n","    \n","    def kl_div(self, mu, logvar):\n","        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n","\n","    def neg_log_likelihood(self, x, mu, logvar, p):\n","        nll = []\n","        for i in range(self.K):\n","            dist = D.Normal(mu[i], torch.exp(0.5*logvar[i]))\n","            nll_i = -dist.log_prob(p[i])\n","            nll.append(nll_i)\n","        nll = torch.stack(nll, dim=1)\n","        return nll\n","\n","    def train_step(self, x, optimizer):\n","        optimizer.zero_grad()\n","        loss, kl, nll = self.loss(x)\n","        loss.backward()\n","        optimizer.step()\n","        return loss, kl, nll\n","\n","\n","\n","\n","dev = \"cpu\"    \n","model = GMVAE(in_size=next(iter(train_loader))[0].shape[-1], x_size=2, hidden_size=256, K=5).to(dev)\n","model.train()\n","opt = optim.Adam(model.parameters())\n","model\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (4096x2766 and 256x2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn [13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m----> 4\u001b[0m         loss, kl, nll \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(batch[\u001b[39m0\u001b[39;49m], opt)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m | Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | KL: \u001b[39m\u001b[39m{\u001b[39;00mkl\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | NLL: \u001b[39m\u001b[39m{\u001b[39;00mnll\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","Cell \u001b[0;32mIn [12], line 87\u001b[0m, in \u001b[0;36mGMVAE.train_step\u001b[0;34m(self, x, optimizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\u001b[39mself\u001b[39m, x, optimizer):\n\u001b[1;32m     86\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 87\u001b[0m     loss, kl, nll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(x)\n\u001b[1;32m     88\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     89\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n","Cell \u001b[0;32mIn [12], line 65\u001b[0m, in \u001b[0;36mGMVAE.loss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 65\u001b[0m     mu_z, logvar_z, mu_px, logvar_px, p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m     66\u001b[0m     kl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkl_div(mu_z, logvar_z)\n\u001b[1;32m     67\u001b[0m     kl \u001b[39m=\u001b[39m kl\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean()\n","Cell \u001b[0;32mIn [12], line 59\u001b[0m, in \u001b[0;36mGMVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     58\u001b[0m     mu_z, logvar_z, z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(x)\n\u001b[0;32m---> 59\u001b[0m     mu_px, logvar_px \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(z)\n\u001b[1;32m     60\u001b[0m     p \u001b[39m=\u001b[39m [D\u001b[39m.\u001b[39mNormal(mu, torch\u001b[39m.\u001b[39mexp(\u001b[39m0.5\u001b[39m\u001b[39m*\u001b[39mlogvar)) \u001b[39mfor\u001b[39;00m mu, logvar \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(mu_px, logvar_px)]\n\u001b[1;32m     61\u001b[0m     p \u001b[39m=\u001b[39m [dist\u001b[39m.\u001b[39mrsample() \u001b[39mfor\u001b[39;00m dist \u001b[39min\u001b[39;00m p]\n","Cell \u001b[0;32mIn [12], line 48\u001b[0m, in \u001b[0;36mGMVAE.decode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, z):\n\u001b[1;32m     47\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecStack(z)\n\u001b[0;32m---> 48\u001b[0m     mu_px \u001b[39m=\u001b[39m [fc(h) \u001b[39mfor\u001b[39;00m fc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_mu_px]\n\u001b[1;32m     49\u001b[0m     logvar_px \u001b[39m=\u001b[39m [fc(h) \u001b[39mfor\u001b[39;00m fc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_logvar_px]\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m mu_px, logvar_px\n","Cell \u001b[0;32mIn [12], line 48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, z):\n\u001b[1;32m     47\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecStack(z)\n\u001b[0;32m---> 48\u001b[0m     mu_px \u001b[39m=\u001b[39m [fc(h) \u001b[39mfor\u001b[39;00m fc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_mu_px]\n\u001b[1;32m     49\u001b[0m     logvar_px \u001b[39m=\u001b[39m [fc(h) \u001b[39mfor\u001b[39;00m fc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_logvar_px]\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m mu_px, logvar_px\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4096x2766 and 256x2)"]}],"source":["# train the GMVAE\n","for epoch in range(100):\n","    for batch in train_loader:\n","        loss, kl, nll = model.train_step(batch[0], opt)\n","    print(f\"Epoch {epoch} | Loss: {loss:.3f} | KL: {kl:.3f} | NLL: {nll:.3f}\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["GMVAE(\n","  (encoder): Sequential(\n","    (0): Linear(in_features=2766, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=256, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=256, out_features=20, bias=True)\n","  )\n","  (decoder): Sequential(\n","    (0): Linear(in_features=2, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=2766, bias=True)\n","  )\n","  (decode): Sequential(\n","    (0): Linear(in_features=2, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=2766, bias=True)\n","  )\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","# from torch.distributions import Gumbel\n","\n","# class GMVAE(nn.Module):\n","#     def __init__(self, input_dim, latent_dim, num_clusters):\n","#         super(GMVAE, self).__init__()\n","#         self.input_dim = input_dim\n","#         self.latent_dim = latent_dim\n","#         self.num_clusters = num_clusters\n","        \n","#         # Encoder network\n","#         self.encoder_fc1 = nn.Linear(input_dim, 2766)\n","#         self.encoder_fc2 = nn.Linear(2766, 512)\n","#         self.encoder_fc3 = nn.Linear(512, latent_dim * num_clusters * 2) # mu and log_var for each cluster\n","        \n","#         # Decoder network\n","#         self.decoder_fc1 = nn.Linear(latent_dim, 512)\n","#         self.decoder_fc2 = nn.Linear(512, 2766)\n","#         self.decoder_fc3 = nn.Linear(2766, input_dim)\n","        \n","#     def encode(self, x):\n","#         h = F.relu(self.encoder_fc1(x))\n","#         h = F.relu(self.encoder_fc2(h))\n","#         h = self.encoder_fc3(h)\n","#         h = h.view(-1, self.num_clusters, self.latent_dim * 2)\n","#         mu, log_var = torch.chunk(h, 2, dim=-1)\n","#         return mu, log_var\n","    \n","#     def sample_gumbel(self, shape, device):\n","#         u = torch.rand(shape).to(device)\n","#         return -torch.log(-torch.log(u + 1e-20) + 1e-20)\n","    \n","#     def gumbel_softmax_sample(self, logits, temperature, device):\n","#         y = logits + self.sample_gumbel(logits.size(), device)\n","#         return F.softmax(y / temperature, dim=-1)\n","    \n","#     def decode(self, z):\n","#         h = F.relu(self.decoder_fc1(z))\n","#         h = F.relu(self.decoder_fc2(h))\n","#         x_hat = F.softmax(self.decoder_fc3(h), dim=-1)\n","#         return x_hat\n","    \n","#     def forward(self, x, temperature=1.0):\n","#         # Encoding\n","#         mu, log_var = self.encode(x)\n","        \n","#         # Sampling from the Gumbel-Softmax distribution\n","#         gumbel_softmax_logits = (mu + self.sample_gumbel(mu.size(), device=x.device)) / temperature\n","#         z = self.gumbel_softmax_sample(gumbel_softmax_logits, temperature, device=x.device)\n","        \n","#         print(z.shape)\n","#         # Decoding\n","#         x_hat = self.decode(z)\n","        \n","#         return x_hat, mu, log_var, z, gumbel_softmax_logits\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch import optim\n","\n","class GMVAE(nn.Module):\n","    def __init__(self, input_dim, latent_dim, num_clusters):\n","        super(GMVAE, self).__init__()\n","        self.input_dim = input_dim\n","        self.latent_dim = latent_dim\n","        self.K = num_clusters\n","        \n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, latent_dim*num_clusters*2)\n","        )\n","        \n","        self.decoder = self.decode = nn.Sequential(\n","            nn.Linear(latent_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, input_dim)\n","        )\n","\n","    def encode(self, x):\n","        h = self.encoder(x)\n","        mu, log_var = torch.chunk(h, 2, dim=-1)\n","        mu = mu.view(-1, self.K, self.latent_dim)\n","        log_var = log_var.view(-1, self.K, self.latent_dim)\n","        return mu, log_var\n","\n","    def reparameterize(self, mu, log_var, temperature=1.0):\n","        eps = torch.randn_like(log_var)\n","        z = mu + torch.exp(log_var / 2) * eps\n","        # Use the Gumbel-Softmax trick to sample z\n","        gumbel_softmax_logits = (z.view(-1, self.K, self.latent_dim) / temperature).softmax(dim=-1)\n","        z = (gumbel_softmax_logits * z.view(-1, self.K, self.latent_dim)).sum(dim=1)\n","        return z, gumbel_softmax_logits\n","\n","\n","    def forward(self, x, temperature=1.0):\n","        x = x.view(-1, self.input_dim)\n","        mu, log_var = self.encode(x)\n","        z, gumbel_softmax_logits = self.reparameterize(mu, log_var, temperature=temperature)\n","        x_hat = self.decode(z)\n","        return x_hat, mu, log_var, z, gumbel_softmax_logits\n","\n","    \n","\n","\n","# initialize the model\n","dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model = GMVAE(input_dim=next(iter(train_loader))[0].shape[-1], latent_dim=2, num_clusters=5).to(dev)\n","model.train()\n","opt = optim.Adam(model.parameters())\n","model"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import torch.optim as optim\n","from tqdm import tqdm\n","\n","def gmvae_loss(x, x_hat, mu, log_var, z, gumbel_softmax_logits, temperature=1.0):\n","    # Reconstruction loss\n","    recon_loss = F.mse_loss(x_hat, x, reduction='sum')\n","\n","    # KL divergence loss\n","    kl_div = (torch.exp(log_var) + mu**2 - 1 - log_var).sum(dim=-1)\n","    kl_loss = torch.mean(torch.sum(gumbel_softmax_logits * kl_div.unsqueeze(-1), dim=-1))\n","    \n","    # Total loss\n","    loss = recon_loss + temperature * kl_loss\n","    \n","    return loss, recon_loss, kl_loss\n","\n","\n","def train(model, dataloader, num_epochs, lr=1e-3, weight_decay=1e-5, initial_temperature=1.0, final_temperature=0.5, device='cpu'): \n","    model.to(device)\n","    model.train()\n","\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    loss_history = {'train_loss': [], 'recon_loss': [], 'kl_loss': []}\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_recon_loss = 0.0\n","        running_kl_loss = 0.0\n","\n","        # Compute the temperature for this epoch based on the linear annealing schedule\n","        temperature = max(initial_temperature - epoch * (initial_temperature - final_temperature) / (num_epochs - 1), final_temperature)\n","        for x, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\", leave=False):\n","            x = x.to(device)\n","            optimizer.zero_grad()\n","            x_hat, mu, log_var, z, gumbel_softmax_logits = model(x, temperature=temperature)\n","            loss, recon_loss, kl_loss = gmvae_loss(x, x_hat, mu, log_var, z, gumbel_softmax_logits, temperature=temperature)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","            running_recon_loss += recon_loss.item()\n","            running_kl_loss += kl_loss.item()\n","\n","        epoch_loss = running_loss / len(dataloader.dataset)\n","        epoch_recon_loss = running_recon_loss / len(dataloader.dataset)\n","        epoch_kl_loss = running_kl_loss / len(dataloader.dataset)\n","\n","        loss_history['train_loss'].append(epoch_loss)\n","        loss_history['recon_loss'].append(epoch_recon_loss)\n","        loss_history['kl_loss'].append(epoch_kl_loss)\n","        tqdm.write(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Recon loss: {epoch_recon_loss:.4f} - KL loss: {epoch_kl_loss:.4f}\")\n","    model.eval()\n","    \n","    return loss_history\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(model, dataloader, num_epochs, initial_temperature, final_temperature, lr, weight_decay, device):\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    train_losses = []\n","    \n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        num_batches = 0\n","        for x, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n","            x = x.to(device)\n","            \n","            # Compute the temperature for this batch based on the linear annealing schedule\n","            temperature = max(initial_temperature - epoch * (initial_temperature - final_temperature) / (num_epochs - 1), final_temperature)\n","            \n","            optimizer.zero_grad()\n","            x_hat, mu, log_var, z, gumbel_softmax_logits = model(x, temperature=temperature)\n","            loss = gmvae_loss(x, x_hat, mu, log_var, z, gumbel_softmax_logits, temperature=temperature)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            train_loss += loss.item()\n","            num_batches += 1\n","        \n","        avg_train_loss = train_loss / num_batches\n","        train_losses.append(avg_train_loss)\n","        print(f\"Train loss: {avg_train_loss:.4f}\")\n","    \n","    return train_losses\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                              \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10 - Loss: 19944.1562 - Recon loss: 19944.1441 - KL loss: 0.0120\n"]},{"name":"stderr","output_type":"stream","text":["                                                              \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10 - Loss: 14839.2559 - Recon loss: 14839.2231 - KL loss: 0.0347\n"]},{"name":"stderr","output_type":"stream","text":["                                                              \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10 - Loss: 12810.9649 - Recon loss: 12810.9251 - KL loss: 0.0446\n"]},{"name":"stderr","output_type":"stream","text":["                                                              \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10 - Loss: 10891.6065 - Recon loss: 10891.5533 - KL loss: 0.0638\n"]},{"name":"stderr","output_type":"stream","text":["                                                              \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10 - Loss: 10246.4142 - Recon loss: 10246.3526 - KL loss: 0.0791\n"]},{"name":"stderr","output_type":"stream","text":["                                                              \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10 - Loss: 9747.1350 - Recon loss: 9747.0628 - KL loss: 0.0998\n"]},{"name":"stderr","output_type":"stream","text":["                                                              \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10 - Loss: 9325.3965 - Recon loss: 9325.3247 - KL loss: 0.1077\n"]},{"name":"stderr","output_type":"stream","text":["                                                              \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10 - Loss: 8903.0704 - Recon loss: 8902.9997 - KL loss: 0.1156\n"]},{"name":"stderr","output_type":"stream","text":["                                                              \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10 - Loss: 8427.9859 - Recon loss: 8427.9170 - KL loss: 0.1241\n"]},{"name":"stderr","output_type":"stream","text":["                                                               "]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10 - Loss: 8051.7168 - Recon loss: 8051.6488 - KL loss: 0.1358\n"]},{"name":"stderr","output_type":"stream","text":["\r"]},{"data":{"text/plain":["{'train_loss': [19944.156183339823,\n","  14839.255899766162,\n","  12810.964850334281,\n","  10891.606483728377,\n","  10246.41416893832,\n","  9747.134962290684,\n","  9325.396527831259,\n","  8903.07036346007,\n","  8427.985908547324,\n","  8051.716794649967],\n"," 'recon_loss': [19944.144131652214,\n","  14839.223124119977,\n","  12810.925122503966,\n","  10891.553337586023,\n","  10246.352569668057,\n","  9747.062762735544,\n","  9325.324684031673,\n","  8902.999656538892,\n","  8427.916994185489,\n","  8051.648832466085],\n"," 'kl_loss': [0.01201444317102654,\n","  0.03467773777556333,\n","  0.04463148100436216,\n","  0.0637854906300506,\n","  0.07905883916774861,\n","  0.09983673322303876,\n","  0.10772955110546911,\n","  0.11564029441548981,\n","  0.12409323372420052,\n","  0.13582139896324355]}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train(model, train_loader, num_epochs=10, initial_temperature=1.0, final_temperature=0.5, lr=1e-3, weight_decay=1e-5, device=dev)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Reconstruction loss: 3.2486\n"]}],"source":["# Calculate reconstruction loss on the complete dataset\n","model.eval()\n","with torch.no_grad():\n","    x, _ = next(iter(train_loader))\n","    x_hat, mu, log_var, z, gumbel_softmax_logits = model(x.to(dev), temperature=1.0)\n","    loss = F.mse_loss(x_hat, x.to(dev), reduction='mean')\n","    print(f\"Reconstruction loss: {loss.item():.4f}\")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["(tensor(169.2537, grad_fn=<SumBackward0>), tensor(156.))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["x_hat, mu, log_var, z, gumbel_softmax_logits = model(x.to(dev), temperature=1.0)\n","\n","x_hat[1].sum(), x[1].sum()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from matplotlib import pyplot as plt\n","# Latent representation of the data\n","model.eval()\n","with torch.no_grad():\n","    # x, _ = next(iter(train_loader))\n","    x_hat, mu, log_var, z, gumbel_softmax_logits = model(x.to(dev), temperature=1.0)\n","    z = z.cpu().numpy()\n","    plt.scatter(z[:, 0], z[:, 1])\n","    plt.show()"]},{"cell_type":"code","execution_count":405,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":125868,"status":"error","timestamp":1675951458486,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"V9WqlbuKaG_-","outputId":"f4733a2f-a460-42eb-a7c8-fabeb8a02610"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_68319/2985712173.py:82: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  qz = F.softmax(self.fc_qz(h1))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 1, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 2, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 3, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 4, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 5, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 6, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 7, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 8, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 9, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 10, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 11, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 12, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 13, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 14, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 15, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 16, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 17, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 18, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 19, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 20, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 21, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 22, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 23, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n","Epoch: 24, MSE: 3.8216352462768555, KLD: 0.9419548511505127\n"]}],"source":["kl_weight = 0.5\n","num_epochs =25\n","\n","# for each epoch\n","for epoch in range(num_epochs):\n","    # epoch losses\n","    mse_e, kld_e = 0, 0\n","    i = 0\n","  # for each data batch\n","    for x, lib in train_loader:\n","        x = x.to(dev)\n","        lib = lib.to(dev)\n","\n","        opt.zero_grad()\n","        \n","        # vae\n","        # recon, mu, logvar = model(x, library=lib)\n","        # mse, kld = model.loss(x, recon, mu, logvar)\n","        # loss = mse + kl_weight * kld\n","        \n","        # gmvae\n","        mu_x, logvar_x, mu_px, logvar_px, qz, recon, mu_w, \\\n","          logvar_w, x_sample = model(x, library=lib)\n","        \n","        loss, recon_loss, KLD_W, \\\n","          KLD_Z, E_KLD_QX_PX, CV = loss_fn(recon, x, mu_w, logvar_w, qz,\tmu_x, logvar_x, mu_px, logvar_px, x_sample, model.x_size, model.K)\n","\n","        loss.backward()\n","        opt.step()\n","        \n","        mse_e += mse\n","        kld_e += kld\n","        i += 1\n","\n","    print(\"Epoch: {}, MSE: {}, KLD: {}\".format(epoch, \n","                                               mse_e/i, \n","                                               kld_e/i))\n","\n","\n","\n"]},{"cell_type":"code","execution_count":205,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1675948590995,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"Jf5-a0W3R7Pt","outputId":"d0c3bd5d-541b-4c38-ae2e-68b95220980f"},"outputs":[{"data":{"text/plain":["tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1.], device='cuda:0')"]},"execution_count":205,"metadata":{},"output_type":"execute_result"}],"source":["# z = model.reparameterize(*model.encoder(test))\n","\n","# for i in range(z.shape[1]):\n","#     model.decoder.zero_grad()\n","#     leaf = z[0].detach().requires_grad_()\n","#     out = model.decoder(z[0])[i]\n","#     out.retain_grad()\n","#     out.backward(retain_graph=True)\n","#     gene_gradients.append(out.grad.clone().detach())\n","\n","\n","# G = torch.stack(gene_gradients)\n","\n","# G"]},{"cell_type":"code","execution_count":161,"metadata":{"executionInfo":{"elapsed":259,"status":"ok","timestamp":1675951467430,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"bCpoOPRr2_7N"},"outputs":[],"source":["# Updated 14/02/2023: \n","\n","from functorch import jacfwd, vmap\n","\n","def jac(f, z):\n","    # composed with vmap for batched Jacobians\n","    return vmap(jacfwd(f))(z)\n","\n","# gene \"correlation\" matrix C\n","def C_matrix(f, z, normalize=False):\n","    '''\n","    f:      model.decode: (b, m) -> (b, n) \n","    z:      torch.tensor whose size = (b, m) (keep b low for memory)\n","    out:    torch.tensor whose size = (b, n, n)\n","    '''\n","    J = jac(f, z)\n","\n","    # J @ J.T on batch \n","    if normalize:\n","        return torch.bmm(J, J.transpose(1,2)) / J.norm(p=2, dim=2).unsqueeze(-1)\n","    else:\n","        return torch.bmm(J, J.transpose(1,2))\n","\n","\n","\n","def jac_robust(f, z):\n","    # alternative jac if experiencing crashes \n","    batch_size, z_dim = z.size()\n","    v = torch.eye(z_dim).unsqueeze(0).repeat(batch_size, 1, 1).view(-1, z_dim).to(z)\n","    z = z.repeat(1, z_dim).view(-1, z_dim)\n","    return torch.autograd.functional.jvp(f, z, v=v)[1].view(batch_size, z_dim, -1).permute(0, 2, 1)\n","\n","\n","# jacrev: 1.45s\n","# jacfwd: 80-90ms \n","# jvp:    85-92ms \n","    # /J.norm(p=2, dim=2).unsqueeze(-1)\n","    # out = torch.einsum('nij,nik->njk', J, J) # J.T @ J, riem. metric "]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":247,"status":"ok","timestamp":1675951541359,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"PTf9PDCQfrQm"},"outputs":[],"source":["# test = next(iter(DataLoader(train_dataset, batch_size=train_dataset.__len__(), shuffle=False)))\n","test = next(iter(DataLoader(train_dataset, batch_size=200, shuffle=False)))\n"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"GMVAE.reparameterize() missing 1 required positional argument: 'log_var'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mreparameterize(model\u001b[39m.\u001b[39;49mencoder(test[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(dev)))\n","\u001b[0;31mTypeError\u001b[0m: GMVAE.reparameterize() missing 1 required positional argument: 'log_var'"]}],"source":["model.reparameterize(model.encoder(test[0].to(dev)))"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"GMVAE.reparameterize() takes from 3 to 4 positional arguments but 201 were given","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Latent representation of the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mreparameterize(\u001b[39m*\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencoder(test[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(dev)))\n\u001b[1;32m      3\u001b[0m z[\u001b[39m0\u001b[39m]\n","\u001b[0;31mTypeError\u001b[0m: GMVAE.reparameterize() takes from 3 to 4 positional arguments but 201 were given"]}],"source":["# Latent representation of the data\n","z = model.encoder(test[0].to(dev)))"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"linear(): argument 'input' (position 1) must be Tensor, not list","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mreparameterize(\u001b[39m*\u001b[39mmodel\u001b[39m.\u001b[39;49mencoder(test))\n\u001b[1;32m      3\u001b[0m z[\u001b[39m0\u001b[39m]\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"]}],"source":["z = model.reparameterize(*model.encoder(test))\n","\n","z[0]"]},{"cell_type":"code","execution_count":333,"metadata":{},"outputs":[{"data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7fdcbd83b2b0>"]},"execution_count":333,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from matplotlib import pyplot as plt\n","\n","zt = z.detach().numpy()\n","\n","\n","plt.scatter(zt[:,0], zt[:,1])"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1221,"status":"ok","timestamp":1675951551346,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"p8gUykks4mUN","outputId":"edd7300d-5cc6-4311-b286-e549b6768dba"},"outputs":[],"source":["z = model.reparameterize(*model.encoder(test))\n","\n","lim = 5\n","# j = JofD(model.decoder, z)[0]\n","# m1 = (j@ j.T) / j.norm(p=2, dim=1).unsqueeze(-1)\n"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5, 2766, 1])\n","CPU times: user 498 ms, sys: 366 ms, total: 863 ms\n","Wall time: 185 ms\n"]},{"data":{"text/plain":["(5, 2766, 2766)"]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","m2 = C_matrix(model.decoder, z[:lim])\n","m2.shape\n","\n","(5, 2766, 2766)"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1675944969376,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"rcYBNWyU3m4P","outputId":"53291031-788e-496e-ce88-b6b76aa8c965"},"outputs":[{"data":{"text/plain":["(tensor([0.9203, 0.4122], grad_fn=<SelectBackward0>),\n"," tensor([-0.3922,  0.7697], grad_fn=<SelectBackward0>))"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["z[0], z[1]"]},{"cell_type":"markdown","metadata":{"id":"bbWh1dYGdaST"},"source":["## Trajectories"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":488,"status":"ok","timestamp":1675957866040,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"AWImcXz03udq"},"outputs":[],"source":["def get_Riemannian_metric(f, z, create_graph=False): #J.T @ J instead! \n","    J = create_jac(f, z, create_graph=create_graph)\n","    out = torch.einsum('nij,nik->njk', J, J)\n","    return out\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1049,"status":"ok","timestamp":1675957803641,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"yWsB3ScDdbZM"},"outputs":[],"source":["def compute_geodesic(z1, z2, pretrained_model, get_Riemannian_metric, num_discretization=100):\n","    '''\n","    z1 : torch.tensor whose size = (1, 2)\n","    z1 : torch.tensor whose size = (1, 2)\n","    out: torch.tensor whose size = (num_discretization, 2)\n","    '''\n","    from scipy.optimize import minimize\n","    class GeodesicFittingTool():\n","        def __init__(self, z1, z2, z_init, pretrained_model, get_Riemannian_metric, num_discretization, method, device=f\"cuda:{0}\"):\n","            self.z1 = z1\n","            self.z2 = z2\n","            self.pretrained_model = pretrained_model\n","            self.get_Riemannian_metric = get_Riemannian_metric\n","            self.num_discretization = num_discretization\n","            self.delta_t = 1/(num_discretization-1)\n","            self.device = device\n","            self.method = method\n","            self.z_init_input = z_init\n","            self.initialize()\n","            \n","        def initialize(self):\n","            self.z_init= self.z1.squeeze(0)\n","            self.z_final= self.z2.squeeze(0)\n","            dim = self.z_final.size(0)\n","            self.init_z = self.z_init_input.detach().cpu().numpy()\n","            self.z_shape = self.init_z.shape\n","            self.init_z_vec = self.init_z.flatten()\n","\n","        def geodesic_loss(self, z): \n","            z_torch = torch.tensor(z.reshape(self.z_shape), dtype=torch.float32).to(self.device)\n","            z_extended = torch.cat([self.z_init.unsqueeze(0), z_torch, self.z_final.unsqueeze(0)], dim=0)\n","            G_ = self.get_Riemannian_metric(self.pretrained_model.decode, z_extended[:-1])\n","            delta_z = (z_extended[1:, :]-z_extended[:-1, :])/(self.delta_t)\n","            loss = torch.einsum('ni, nij, nj -> ', delta_z, G_, delta_z) * self.delta_t\n","            return loss.item()\n","        \n","        def jac(self, z):\n","            z_torch = torch.tensor(z.reshape(self.z_shape), dtype=torch.float32).to(self.device)\n","            z_torch.requires_grad = True\n","            z_extended = torch.cat([self.z_init.unsqueeze(0), z_torch, self.z_final.unsqueeze(0)], dim=0)\n","            G_ = self.get_Riemannian_metric(self.pretrained_model.decode, z_extended[:-1], create_graph=True)\n","            delta_z = (z_extended[1:, :]-z_extended[:-1, :])/(self.delta_t)\n","            loss = torch.einsum('ni, nij, nj -> ', delta_z, G_, delta_z) * self.delta_t\n","            loss.backward()\n","            z_grad = z_torch.grad\n","            return z_grad.detach().cpu().numpy().flatten()\n","\n","        def callback(self, z):\n","            self.Nfeval += 1\n","            return print('{} th loss : {}'.format(self.Nfeval, self.geodesic_loss(z)))\n","            \n","        def BFGS_optimizer(self, callback=False, maxiter=1000):\n","            self.Nfeval = 0\n","            z0 = self.init_z_vec\n","            if callback == True:\n","                call = self.callback\n","            else:\n","                call = None\n","            res = minimize(\n","                self.geodesic_loss, \n","                z0, \n","                callback=call, \n","                method=self.method,\n","                jac = self.jac,\n","                options = {\n","                    'gtol': 1e-10, \n","                    'eps': 1.4901161193847656e-08, \n","                    'maxiter': maxiter, \n","                    'disp': True, \n","                    'return_all': False, \n","                    'finite_diff_rel_step': None}\n","                )\n","            self.res = res\n","\n","    z12_linear_curve = torch.cat([z1.to(device) + (z2.to(device) - z1.to(device)) * t/(num_discretization-1) for t in range(num_discretization)], dim=0)\n","    \n","    tool = GeodesicFittingTool(z1, z2, z12_linear_curve[1:-1], pretrained_model, get_Riemannian_metric, num_discretization, 'BFGS', device=device)\n","    tool.BFGS_optimizer()\n","    z_torch = torch.tensor(tool.res['x'].reshape(tool.z_shape), dtype=torch.float32).to(device)\n","    out = torch.cat([tool.z_init.unsqueeze(0), z_torch, tool.z_final.unsqueeze(0)], dim=0)\n","    return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TbwaJy63ktw"},"outputs":[],"source":["z1 = torch.tensor([[-1, 1]], dtype=torch.float32) \n","z2 = torch.tensor([[0.6, -1.2]], dtype=torch.float32) \n","\n","num_discretization = 100\n","z12_geodesic_curve = compute_geodesic(\n","    z1.to(device), \n","    z2.to(device), \n","    pretrained_model, \n","    get_Riemannian_metric, \n","    num_discretization=num_discretization\n",")\n","assert z12_geodesic_curve.size() == torch.Size([num_discretization, 2])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOkivSru5FLc"},"outputs":[],"source":["L12 = compute_length_of_curve(\n","    z12_geodesic_curve, pretrained_model, get_Riemannian_metric)\n","L12\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQ3qhCoe5zF4"},"outputs":[],"source":["z12_linear_curve = torch.cat([z1.to(device) + (z2.to(device) - z1.to(device)) * t/(num_discretization-1) for t in range(num_discretization)], dim=0)\n","\n","latent_embeddings = pretrained_model.encode(\n","    train_ds.data.to(device)\n","    ).detach().cpu().numpy()\n","\n","z_scale = np.minimum(\n","    np.max(latent_embeddings, axis=0), \n","    np.min(latent_embeddings, axis=0)\n",")\n","labels = torch.unique(train_ds.targets)\n","\n","f = plt.figure(figsize=(7, 7))\n","plt.title('Latent space embeddings')\n","for label in labels:\n","    classwise_le = latent_embeddings[train_ds.targets == label]\n","    plt.scatter(\n","        classwise_le[:200, 0], \n","        classwise_le[:200, 1], \n","        label=label.item(), s=5)\n","\n","plt.scatter(z1[0, 0], z1[0, 1], c='tab:red', marker='s')\n","plt.scatter(z2[0, 0], z2[0, 1], c='tab:green', marker='s')\n","\n","plt.scatter(z12_linear_curve[:, 0].detach().cpu(), z12_linear_curve[:, 1].detach().cpu(), s=10, c='tab:gray')\n","plt.scatter(z12_geodesic_curve[:, 0].detach().cpu(), z12_geodesic_curve[:, 1].detach().cpu(), s=10, c='tab:pink')\n","\n","plt.xlim(-3, 3)\n","plt.ylim(-3, 3)\n","\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJOoNKJr5zrN"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOf/+AyiK5BjkA10sLO5KCd","mount_file_id":"1gEzMrTlr7G_N89-yMbnXE_sKWSHFhCj9","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":0}
