{"cells":[{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1676942417432,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"oSbu8jTsZh8s","outputId":"8edc59cb-5ab8-46dc-b63d-b73624e0309d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/cells\n"]}],"source":["cd /content/drive/MyDrive/Colab Notebooks/cells"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3072,"status":"ok","timestamp":1676942421717,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"0GB9rFhVYcsW","outputId":"a46c34c4-b3a1-4700-80a6-5bc9bfa51b49"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchinfo in /usr/local/lib/python3.8/dist-packages (1.7.2)\n"]}],"source":["!pip install torchinfo"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18260,"status":"ok","timestamp":1676942439969,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"NC2-tZ5QZsmw","outputId":"87e74ddc-1695-4b18-e973-e0bf657a0f95"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([86024, 2766])\n"]}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.io import mmread\n","\n","\n","class RNA(Dataset):\n","    def __init__(self, data_file):\n","        # load the scRNA-seq data from the specified file\n","        self.data = torch.from_numpy(\n","            mmread(data_file).astype(\"float32\").transpose().todense())\n","        print(self.data.shape)\n","        \n","    def __len__(self):\n","        # return the number of examples in the dataset\n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        # return the preprocessed data for the specified example\n","        library = self.data[index].sum(dim=-1)\n","        example = self.data[index]\n","        return example, library\n","\n","        \n","\n","# datasets: hcl, celegan, uc_epi, zfish_ep50_5226\n","# train_dataset = RNA(\"cells/data/hcl.mtx\")\n","# train_dataset = RNA(\"data/zfish_ep50_5226.mtx\")\n","train_dataset = RNA(\"cells/data/celegan.mtx\")\n","train_loader = DataLoader(train_dataset, batch_size=2**12, shuffle=True) # 2**15\n"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1676942439970,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"oeKzvTc0Z1Il"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","\n","# ---------------------------\n","# - Variational Autoencoder -\n","# ---------------------------\n","class VAE(nn.Module):\n","    def __init__(self, input_dim, latent_dim):\n","        super(VAE, self).__init__()\n","        self.input_dim = input_dim\n","        self.latent_dim = latent_dim\n","        \n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, latent_dim * 2) # mu + log_var\n","        )\n","        \n","        self.decoder = self.decode = nn.Sequential(\n","            nn.Linear(latent_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, input_dim))\n","        \n","    def encode(self, x):\n","        h = self.encoder(x)\n","        mu, log_var = torch.chunk(h, 2, dim=-1)\n","        return mu, log_var\n","    \n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(0.5 * log_var)\n","        eps = torch.randn_like(std)\n","        z = mu + eps * std\n","        return z\n","    \n","    def forward(self, x):\n","        mu, log_var = self.encode(x)\n","        z = self.reparameterize(mu, log_var)\n","        x_hat = self.decode(z)\n","        return x_hat, mu, log_var\n","\n","    def loss(self, x, x_hat, mu, log_var):\n","        recon_loss = F.mse_loss(x_hat, x, reduction=\"sum\")\n","        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","        return recon_loss, kl_div\n","\n","\n","dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","vae = VAE(input_dim=next(iter(train_loader))[0].shape[-1], latent_dim=2).to(dev)\n","vae_opt = optim.Adam(vae.parameters(), lr=1e-3)\n","vae.train()\n","opt = optim.Adam(vae.parameters())"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":544149,"status":"ok","timestamp":1676942984093,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"V9WqlbuKaG_-","outputId":"7404fd97-9f6a-4742-fc36-8e38421d3d16"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss: 28354.178428403702, Recon Loss: 27560.361446369618, KL Loss: 198.45422499427949\n","Epoch: 1, Loss: 23045.431315985854, Recon Loss: 23027.481218427234, KL Loss: 4.4875381592538846\n","Epoch: 2, Loss: 22459.355449496652, Recon Loss: 22441.743319542365, KL Loss: 4.40300959107555\n","Epoch: 3, Loss: 22070.729127599927, Recon Loss: 22055.051719305804, KL Loss: 3.9194166653878875\n","Epoch: 4, Loss: 22835.11803805297, Recon Loss: 22811.41565191155, KL Loss: 5.92559358444334\n","Epoch: 5, Loss: 23080.36002094074, Recon Loss: 23067.3829270665, KL Loss: 3.24427040339647\n","Epoch: 6, Loss: 24150.095748261385, Recon Loss: 24137.9549168328, KL Loss: 3.0352286959896198\n","Epoch: 7, Loss: 22695.07529539358, Recon Loss: 22681.942664619757, KL Loss: 3.2831262320925845\n","Epoch: 8, Loss: 22486.518312612978, Recon Loss: 22471.37478408097, KL Loss: 3.7858622834414843\n","Epoch: 9, Loss: 23279.322440358213, Recon Loss: 23257.278831888485, KL Loss: 5.510917569475041\n","Epoch: 10, Loss: 23638.441156932768, Recon Loss: 23624.654071488225, KL Loss: 3.44677065650574\n","Epoch: 11, Loss: 22105.089317518312, Recon Loss: 22092.850682681034, KL Loss: 3.059657346267739\n","Epoch: 12, Loss: 21297.424336704, Recon Loss: 21281.344881315825, KL Loss: 4.019853533716698\n","Epoch: 13, Loss: 20052.820160970703, Recon Loss: 20032.841700676374, KL Loss: 4.994586736204945\n","Epoch: 14, Loss: 21954.769104951814, Recon Loss: 21923.20284364683, KL Loss: 7.891518953533203\n","Epoch: 15, Loss: 21275.870893542426, Recon Loss: 21249.24049343243, KL Loss: 6.657593514213537\n","Epoch: 16, Loss: 19521.670642240828, Recon Loss: 19500.047996961603, KL Loss: 5.4056953912467325\n","Epoch: 17, Loss: 18747.15505315922, Recon Loss: 18730.07846538466, KL Loss: 4.269145712900814\n","Epoch: 18, Loss: 19474.179376154294, Recon Loss: 19454.311713064813, KL Loss: 4.96692848764308\n","Epoch: 19, Loss: 21090.8714137915, Recon Loss: 21071.110602855017, KL Loss: 4.940169498929975\n","Epoch: 20, Loss: 22123.923675369664, Recon Loss: 22106.48812322724, KL Loss: 4.358964932150013\n","Epoch: 21, Loss: 22667.90691814566, Recon Loss: 22648.049713234097, KL Loss: 4.964291093782482\n","Epoch: 22, Loss: 19969.213696722716, Recon Loss: 19950.110751387332, KL Loss: 4.775729729077611\n","Epoch: 23, Loss: 17916.344665215158, Recon Loss: 17893.028889591766, KL Loss: 5.828929601797201\n","Epoch: 24, Loss: 17857.14505892342, Recon Loss: 17832.267950182326, KL Loss: 6.219304409601805\n","Epoch: 25, Loss: 17295.617308242105, Recon Loss: 17272.71181270997, KL Loss: 5.726416750042026\n","Epoch: 26, Loss: 16492.981729481016, Recon Loss: 16465.22137254211, KL Loss: 6.940100965195674\n","Epoch: 27, Loss: 16352.806913486716, Recon Loss: 16327.349495444221, KL Loss: 6.364315914972476\n","Epoch: 28, Loss: 15477.829567366374, Recon Loss: 15451.798150625116, KL Loss: 6.50786407628215\n","Epoch: 29, Loss: 15112.183318754069, Recon Loss: 15079.239115841763, KL Loss: 8.23606573552029\n","Epoch: 30, Loss: 14895.283024940714, Recon Loss: 14863.851391762764, KL Loss: 7.857910333317867\n","Epoch: 31, Loss: 14696.128276656442, Recon Loss: 14662.864361026792, KL Loss: 8.315976232193265\n","Epoch: 32, Loss: 14600.321957435208, Recon Loss: 14570.618616243722, KL Loss: 7.425837886396657\n","Epoch: 33, Loss: 18277.370926600062, Recon Loss: 18250.84127063196, KL Loss: 6.632413103005548\n","Epoch: 34, Loss: 14551.387058862207, Recon Loss: 14523.492241627337, KL Loss: 6.9737167313166974\n","Epoch: 35, Loss: 14524.598622323057, Recon Loss: 14496.042157112404, KL Loss: 7.1391172539874095\n","Epoch: 36, Loss: 14697.591986683949, Recon Loss: 14665.244704148698, KL Loss: 8.086814888353116\n","Epoch: 37, Loss: 17463.41258090039, Recon Loss: 17431.56243933379, KL Loss: 7.962502928763714\n","Epoch: 38, Loss: 13919.506469669801, Recon Loss: 13881.434993068795, KL Loss: 9.517884184834593\n","Epoch: 39, Loss: 12865.005771917575, Recon Loss: 12821.236458539186, KL Loss: 10.942356584815549\n","Epoch: 40, Loss: 12107.667811907868, Recon Loss: 12064.3546380861, KL Loss: 10.828304173879198\n","Epoch: 41, Loss: 12089.264844694504, Recon Loss: 12046.161731664273, KL Loss: 10.775762226286018\n","Epoch: 42, Loss: 11514.098796829228, Recon Loss: 11467.863902306697, KL Loss: 11.558686940645401\n","Epoch: 43, Loss: 13100.909060080618, Recon Loss: 13050.911180037692, KL Loss: 12.499489258757572\n","Epoch: 44, Loss: 12927.946864664947, Recon Loss: 12887.234777413134, KL Loss: 10.178035623626156\n","Epoch: 45, Loss: 13434.89684900798, Recon Loss: 13394.612192164395, KL Loss: 10.071169079929842\n","Epoch: 46, Loss: 13125.587829050613, Recon Loss: 13081.854203478098, KL Loss: 10.933394678978436\n","Epoch: 47, Loss: 13551.107211654778, Recon Loss: 13505.635504177068, KL Loss: 11.36793099968065\n","Epoch: 48, Loss: 12198.970627564691, Recon Loss: 12150.09500351101, KL Loss: 12.218888486849588\n","Epoch: 49, Loss: 11634.069572335919, Recon Loss: 11578.590847648622, KL Loss: 13.869710332635812\n","Epoch: 50, Loss: 11798.577817809202, Recon Loss: 11741.175181263005, KL Loss: 14.350659230027455\n","Epoch: 51, Loss: 10676.08123314424, Recon Loss: 10615.599833240578, KL Loss: 15.12035638122218\n","Epoch: 52, Loss: 10136.963914571625, Recon Loss: 10077.253199257075, KL Loss: 14.927669609223788\n","Epoch: 53, Loss: 9721.857896272497, Recon Loss: 9660.882870214635, KL Loss: 15.243762917466732\n","Epoch: 54, Loss: 9487.307825477847, Recon Loss: 9429.472928339766, KL Loss: 14.458722149595198\n","Epoch: 55, Loss: 9297.26125553443, Recon Loss: 9239.034510808757, KL Loss: 14.55667550768019\n","Epoch: 56, Loss: 8957.780709362358, Recon Loss: 8892.961150921545, KL Loss: 16.204882847656375\n","Epoch: 57, Loss: 8754.18629056962, Recon Loss: 8688.414836062284, KL Loss: 16.44283774379552\n","Epoch: 58, Loss: 8457.286361582233, Recon Loss: 8387.298455954153, KL Loss: 17.496987034196266\n","Epoch: 59, Loss: 8320.560230012119, Recon Loss: 8249.700447785646, KL Loss: 17.714953818543393\n","Epoch: 60, Loss: 8479.911279301416, Recon Loss: 8409.922064808949, KL Loss: 17.497281185119284\n","Epoch: 61, Loss: 8289.380002419382, Recon Loss: 8220.253206953874, KL Loss: 17.28168797251303\n","Epoch: 62, Loss: 8232.527533150269, Recon Loss: 8158.997920275156, KL Loss: 18.382391590745264\n","Epoch: 63, Loss: 8160.836053292541, Recon Loss: 8083.814284658206, KL Loss: 19.2554643154366\n","Epoch: 64, Loss: 8541.516309583663, Recon Loss: 8467.757641808174, KL Loss: 18.43967666704023\n","Epoch: 65, Loss: 8413.412687429525, Recon Loss: 8337.658142731243, KL Loss: 18.938638349228167\n","Epoch: 66, Loss: 8323.151726280603, Recon Loss: 8247.159145903324, KL Loss: 18.998156994983095\n","Epoch: 67, Loss: 8004.574215480564, Recon Loss: 7930.01042137405, KL Loss: 18.6409528838372\n","Epoch: 68, Loss: 7834.061499665972, Recon Loss: 7760.396114031858, KL Loss: 18.416364117355155\n","Epoch: 69, Loss: 7705.171398587486, Recon Loss: 7632.946383427744, KL Loss: 18.056269410930767\n","Epoch: 70, Loss: 7683.756498822277, Recon Loss: 7607.802397986317, KL Loss: 18.988532110867162\n","Epoch: 71, Loss: 7913.240827346329, Recon Loss: 7836.00147619596, KL Loss: 19.309828524366484\n","Epoch: 72, Loss: 7795.329379294314, Recon Loss: 7717.489286465478, KL Loss: 19.460021027052644\n","Epoch: 73, Loss: 7633.33451442103, Recon Loss: 7552.919722768592, KL Loss: 20.10370699611917\n","Epoch: 74, Loss: 7832.823935603182, Recon Loss: 7754.351028401231, KL Loss: 19.618224620153924\n","Epoch: 75, Loss: 7834.114339457593, Recon Loss: 7759.209380249262, KL Loss: 18.726224358642998\n","Epoch: 76, Loss: 7594.119577503444, Recon Loss: 7515.40719712622, KL Loss: 19.678090189885264\n","Epoch: 77, Loss: 7584.8371368609705, Recon Loss: 7506.04859885577, KL Loss: 19.697135683703753\n","Epoch: 78, Loss: 7586.00312474117, Recon Loss: 7504.996063189851, KL Loss: 20.251749950952778\n","Epoch: 79, Loss: 7796.110608735462, Recon Loss: 7719.101096423672, KL Loss: 19.25238516146024\n","Epoch: 80, Loss: 7462.882763276818, Recon Loss: 7384.186111942598, KL Loss: 19.674155476613677\n","Epoch: 81, Loss: 7502.402383346508, Recon Loss: 7421.8759222989365, KL Loss: 20.13159800937184\n","Epoch: 82, Loss: 7543.020021177321, Recon Loss: 7459.254448227202, KL Loss: 20.9413888773053\n","Epoch: 83, Loss: 7549.986230904675, Recon Loss: 7469.6926037492085, KL Loss: 20.07339588879408\n","Epoch: 84, Loss: 7310.661274828645, Recon Loss: 7230.942223086908, KL Loss: 19.929767568555086\n","Epoch: 85, Loss: 7182.077383700697, Recon Loss: 7099.598353673012, KL Loss: 20.61975169388026\n","Epoch: 86, Loss: 7113.619818942243, Recon Loss: 7030.319524179844, KL Loss: 20.825077507425593\n","Epoch: 87, Loss: 7082.167409313099, Recon Loss: 6998.792708738985, KL Loss: 20.843672325342165\n","Epoch: 88, Loss: 7111.620154087858, Recon Loss: 7029.311600859571, KL Loss: 20.577139851327033\n","Epoch: 89, Loss: 7089.7432309043115, Recon Loss: 7008.147009301183, KL Loss: 20.399060851882627\n","Epoch: 90, Loss: 7020.666156074541, Recon Loss: 6941.784406777106, KL Loss: 19.720448768450407\n","Epoch: 91, Loss: 6863.585521191761, Recon Loss: 6782.880679555996, KL Loss: 20.17620805903367\n","Epoch: 92, Loss: 6719.380561595183, Recon Loss: 6640.190004376958, KL Loss: 19.797636307572958\n","Epoch: 93, Loss: 6632.205043364534, Recon Loss: 6551.11607809394, KL Loss: 20.272244131932045\n","Epoch: 94, Loss: 7154.855151308156, Recon Loss: 7074.127570548989, KL Loss: 20.1819021816\n","Epoch: 95, Loss: 7725.991654173181, Recon Loss: 7644.668967562831, KL Loss: 20.330688091474258\n","Epoch: 96, Loss: 7883.554214385303, Recon Loss: 7813.4175372979125, KL Loss: 17.534171906801795\n","Epoch: 97, Loss: 7406.333733196005, Recon Loss: 7346.656951816317, KL Loss: 14.919192441328175\n","Epoch: 98, Loss: 7596.47896789704, Recon Loss: 7536.276154946585, KL Loss: 15.050708007936665\n","Epoch: 99, Loss: 7658.713799312637, Recon Loss: 7598.453452170688, KL Loss: 15.065076023769814\n","Epoch: 100, Loss: 7268.558055973115, Recon Loss: 7205.9679862914345, KL Loss: 15.647522685539553\n","Epoch: 101, Loss: 7053.1637405910615, Recon Loss: 6987.691413277018, KL Loss: 16.36807996639371\n","Epoch: 102, Loss: 6999.012042060936, Recon Loss: 6932.745039447567, KL Loss: 16.5667452920839\n","Epoch: 103, Loss: 6885.495562216831, Recon Loss: 6818.168599342553, KL Loss: 16.831729547462828\n","Epoch: 104, Loss: 6848.990324625068, Recon Loss: 6780.720630856832, KL Loss: 17.067418675549938\n","Epoch: 105, Loss: 6761.740296448884, Recon Loss: 6691.759353903053, KL Loss: 17.495236996239445\n","Epoch: 106, Loss: 6767.729451898507, Recon Loss: 6696.825127437637, KL Loss: 17.72608075265628\n","Epoch: 107, Loss: 6674.841753702546, Recon Loss: 6603.447817807839, KL Loss: 17.848482884396667\n","Epoch: 108, Loss: 6647.699935483121, Recon Loss: 6575.50657065906, KL Loss: 18.048328946692816\n","Epoch: 109, Loss: 6533.6991300551845, Recon Loss: 6459.40284951933, KL Loss: 18.574076853762513\n","Epoch: 110, Loss: 6486.3963155449355, Recon Loss: 6413.961261151503, KL Loss: 18.10875969213363\n","Epoch: 111, Loss: 6627.913613406578, Recon Loss: 6556.06220579613, KL Loss: 17.962845272212142\n","Epoch: 112, Loss: 7432.807703294012, Recon Loss: 7370.880510862157, KL Loss: 15.481791250696629\n","Epoch: 113, Loss: 7130.935933213405, Recon Loss: 7068.695480240252, KL Loss: 15.560098173229637\n","Epoch: 114, Loss: 6618.671917184812, Recon Loss: 6556.284967235706, KL Loss: 15.596733675771986\n","Epoch: 115, Loss: 6443.095312949547, Recon Loss: 6377.76517838136, KL Loss: 16.33254467674957\n","Epoch: 116, Loss: 6415.041918557067, Recon Loss: 6347.136774194193, KL Loss: 16.976284637813023\n","Epoch: 117, Loss: 6310.02167413854, Recon Loss: 6241.533401607037, KL Loss: 17.12207612288028\n","Epoch: 118, Loss: 6379.300798005862, Recon Loss: 6313.675379359067, KL Loss: 16.40637482712571\n","Epoch: 119, Loss: 6357.534433477504, Recon Loss: 6295.081195068927, KL Loss: 15.613294117907241\n","Epoch: 120, Loss: 6266.02589945375, Recon Loss: 6203.59480012482, KL Loss: 15.607766796286956\n","Epoch: 121, Loss: 6128.315093503157, Recon Loss: 6064.223877770484, KL Loss: 16.022807066733034\n","Epoch: 122, Loss: 6308.85559080896, Recon Loss: 6246.936476575578, KL Loss: 15.47979604411899\n","Epoch: 123, Loss: 6491.890860308606, Recon Loss: 6429.290330768884, KL Loss: 15.650136294525014\n","Epoch: 124, Loss: 8414.409507607434, Recon Loss: 8353.101521212013, KL Loss: 15.327007905301478\n","Epoch: 125, Loss: 10008.440063011121, Recon Loss: 9967.943522665186, KL Loss: 10.124118146858226\n","Epoch: 126, Loss: 8877.326507105758, Recon Loss: 8839.121930294345, KL Loss: 9.551142434657901\n","Epoch: 127, Loss: 8351.35878870377, Recon Loss: 8312.189387849794, KL Loss: 9.792350755916976\n","Epoch: 128, Loss: 8394.766094533075, Recon Loss: 8353.344977707438, KL Loss: 10.35528397389393\n","Epoch: 129, Loss: 7986.3376803026085, Recon Loss: 7942.085971692856, KL Loss: 11.062896182456594\n","Epoch: 130, Loss: 8199.770030565598, Recon Loss: 8153.345464138279, KL Loss: 11.606146874255295\n","Epoch: 131, Loss: 9248.22571471697, Recon Loss: 9201.603512764243, KL Loss: 11.655563654616236\n","Epoch: 132, Loss: 9510.646262257662, Recon Loss: 9459.54192411965, KL Loss: 12.776094205698989\n","Epoch: 133, Loss: 8491.5870306724, Recon Loss: 8441.188815039988, KL Loss: 12.599567345748953\n","Epoch: 134, Loss: 8599.185715863996, Recon Loss: 8545.612470747627, KL Loss: 13.393312916471428\n","Epoch: 135, Loss: 8898.309103418813, Recon Loss: 8845.573662537126, KL Loss: 13.183855912080755\n","Epoch: 136, Loss: 7829.247031215489, Recon Loss: 7772.997943467721, KL Loss: 14.062256497759426\n","Epoch: 137, Loss: 7600.972018346624, Recon Loss: 7541.825821137125, KL Loss: 14.786545296996003\n","Epoch: 138, Loss: 7729.210424470933, Recon Loss: 7668.209312476569, KL Loss: 15.250273777342592\n","Epoch: 139, Loss: 8056.988344549922, Recon Loss: 7995.032916686041, KL Loss: 15.488874534756642\n","Epoch: 140, Loss: 7396.867103902327, Recon Loss: 7333.320174275493, KL Loss: 15.886722240095015\n","Epoch: 141, Loss: 7000.228237007714, Recon Loss: 6934.810663314616, KL Loss: 16.35439819049294\n","Epoch: 142, Loss: 6839.529350412022, Recon Loss: 6774.111565522412, KL Loss: 16.354441819163927\n","Epoch: 143, Loss: 6620.26893262527, Recon Loss: 6553.949210712635, KL Loss: 16.57993692887458\n","Epoch: 144, Loss: 7068.520338471586, Recon Loss: 7002.430428106818, KL Loss: 16.52246651178979\n","Epoch: 145, Loss: 8779.266164774874, Recon Loss: 8711.135015532547, KL Loss: 17.032795848152887\n","Epoch: 146, Loss: 11516.84673506358, Recon Loss: 11450.252776637008, KL Loss: 16.64847198694925\n","Epoch: 147, Loss: 9190.669687588093, Recon Loss: 9123.034810302757, KL Loss: 16.908702700811908\n","Epoch: 148, Loss: 7365.289062409182, Recon Loss: 7298.0304599770125, KL Loss: 16.814665270042365\n","Epoch: 149, Loss: 7152.66238303137, Recon Loss: 7084.520461291144, KL Loss: 17.03548869964279\n","Epoch: 150, Loss: 7301.437644513631, Recon Loss: 7232.463269561765, KL Loss: 17.243585200572717\n","Epoch: 151, Loss: 6957.291306786478, Recon Loss: 6889.450383214277, KL Loss: 16.960221987596327\n","Epoch: 152, Loss: 7141.818786660827, Recon Loss: 7072.752601234012, KL Loss: 17.266560433709788\n","Epoch: 153, Loss: 7278.968759808309, Recon Loss: 7209.031065095206, KL Loss: 17.484434330018853\n","Epoch: 154, Loss: 8049.596165750256, Recon Loss: 7978.455239962103, KL Loss: 17.78523338826592\n","Epoch: 155, Loss: 8452.460190524593, Recon Loss: 8381.25172499099, KL Loss: 17.802121915828792\n","Epoch: 156, Loss: 9406.912584056303, Recon Loss: 9335.272606767987, KL Loss: 17.910010759191692\n","Epoch: 157, Loss: 8878.347010627122, Recon Loss: 8815.153584909807, KL Loss: 15.798365917381416\n","Epoch: 158, Loss: 8478.876721358283, Recon Loss: 8424.786302459925, KL Loss: 13.52260967149276\n","Epoch: 159, Loss: 8644.49571068103, Recon Loss: 8593.299019387032, KL Loss: 12.79918336934803\n","Epoch: 160, Loss: 8990.899458472344, Recon Loss: 8942.988045214854, KL Loss: 11.977874842419975\n","Epoch: 161, Loss: 9507.899565841546, Recon Loss: 9458.259472579055, KL Loss: 12.410002156433855\n","Epoch: 162, Loss: 8468.805599786288, Recon Loss: 8414.722642491122, KL Loss: 13.520735009419667\n","Epoch: 163, Loss: 8759.525537931275, Recon Loss: 8706.34967276574, KL Loss: 13.293987094485725\n","Epoch: 164, Loss: 9754.711629133384, Recon Loss: 9703.721921239821, KL Loss: 12.747427882632023\n","Epoch: 165, Loss: 8373.111737644618, Recon Loss: 8318.06822298955, KL Loss: 13.760858094626823\n","Epoch: 166, Loss: 7676.252639059576, Recon Loss: 7619.321604903683, KL Loss: 14.232764261640039\n","Epoch: 167, Loss: 7347.151387962044, Recon Loss: 7289.681081119986, KL Loss: 14.367577389518704\n","Epoch: 168, Loss: 7311.544374970938, Recon Loss: 7252.995319461175, KL Loss: 14.637274682261296\n","Epoch: 169, Loss: 7024.315347883476, Recon Loss: 6963.52869007668, KL Loss: 15.196661408774789\n","Epoch: 170, Loss: 6861.787065816571, Recon Loss: 6800.279827068609, KL Loss: 15.376812003728213\n","Epoch: 171, Loss: 6590.776181102076, Recon Loss: 6529.232641835418, KL Loss: 15.385892580157046\n","Epoch: 172, Loss: 6485.621807304502, Recon Loss: 6423.435485959042, KL Loss: 15.546576337903907\n","Epoch: 173, Loss: 6428.2323509918015, Recon Loss: 6365.467195246748, KL Loss: 15.691284853015446\n","Epoch: 174, Loss: 6356.031628959469, Recon Loss: 6292.698852663937, KL Loss: 15.833201885267734\n","Epoch: 175, Loss: 6328.6095967767715, Recon Loss: 6264.773393816697, KL Loss: 15.959063640563619\n","Epoch: 176, Loss: 6328.123799844375, Recon Loss: 6264.185317741996, KL Loss: 15.984607361110879\n","Epoch: 177, Loss: 6290.469620521508, Recon Loss: 6225.342890194978, KL Loss: 16.281690027795147\n","Epoch: 178, Loss: 6233.10305549723, Recon Loss: 6167.92676644018, KL Loss: 16.294060681638502\n","Epoch: 179, Loss: 6194.419302316795, Recon Loss: 6129.393841398912, KL Loss: 16.256355780707306\n","Epoch: 180, Loss: 6154.028380070533, Recon Loss: 6088.536809313389, KL Loss: 16.37288146283719\n","Epoch: 181, Loss: 6141.965732855076, Recon Loss: 6075.889587362611, KL Loss: 16.51903215133566\n","Epoch: 182, Loss: 6159.482452355684, Recon Loss: 6093.429263563075, KL Loss: 16.513293520745528\n","Epoch: 183, Loss: 6098.342299550852, Recon Loss: 6031.761023881235, KL Loss: 16.64531138015771\n","Epoch: 184, Loss: 6149.885301426601, Recon Loss: 6083.478601152549, KL Loss: 16.601685692939625\n","Epoch: 185, Loss: 6073.043239545614, Recon Loss: 6005.85236368221, KL Loss: 16.797725232270754\n","Epoch: 186, Loss: 6085.158524131438, Recon Loss: 6017.459401931983, KL Loss: 16.924776917511416\n","Epoch: 187, Loss: 6038.233933052795, Recon Loss: 5970.5347235661975, KL Loss: 16.92478838546059\n","Epoch: 188, Loss: 5987.26059510827, Recon Loss: 5919.625037326066, KL Loss: 16.908891535599064\n","Epoch: 189, Loss: 5974.158701010219, Recon Loss: 5906.337658722057, KL Loss: 16.955260029263044\n","Epoch: 190, Loss: 5953.398805947323, Recon Loss: 5885.198805166291, KL Loss: 17.050001199307758\n","Epoch: 191, Loss: 5914.780786693612, Recon Loss: 5846.161937366316, KL Loss: 17.154711334248383\n","Epoch: 192, Loss: 5923.525595146412, Recon Loss: 5854.553497652181, KL Loss: 17.243029367820636\n","Epoch: 193, Loss: 5926.207190249052, Recon Loss: 5857.006950231549, KL Loss: 17.300049196539685\n","Epoch: 194, Loss: 5907.162246782148, Recon Loss: 5838.049717320893, KL Loss: 17.278124371052098\n","Epoch: 195, Loss: 5851.06623477487, Recon Loss: 5781.342217701419, KL Loss: 17.431003177486375\n","Epoch: 196, Loss: 5862.463747103824, Recon Loss: 5792.683507007765, KL Loss: 17.445058387877676\n","Epoch: 197, Loss: 5917.003938467572, Recon Loss: 5846.5581730825115, KL Loss: 17.61144806411248\n","Epoch: 198, Loss: 6092.591032178792, Recon Loss: 6022.301941729652, KL Loss: 17.57226107790782\n","Epoch: 199, Loss: 6096.203878900256, Recon Loss: 6025.671810110768, KL Loss: 17.63302482410574\n"]}],"source":["def train_vae(model, opt, train_loader, num_epochs, kl_weight=4):\n","    loss_history = {'train_loss': [], 'recon_loss': [], 'kl_loss': []}\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_recon_loss = 0.0\n","        running_kl_loss = 0.0\n","\n","        for _, (data, library) in enumerate(train_loader):\n","            data = data.to(dev)\n","            library = library.to(dev)\n","            opt.zero_grad()\n","            x_hat, mu, log_var = model(data)\n","            x_hat = F.softmax(x_hat, dim=-1) * library.unsqueeze(-1)\n","            recon_loss, kl_div = model.loss(data, x_hat, mu, log_var)\n","            loss = recon_loss + kl_weight * kl_div\n","            loss.backward()\n","            opt.step()\n","\n","            running_loss += loss.item()\n","            running_recon_loss += recon_loss.item()\n","            running_kl_loss += kl_div.item()\n","\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_recon_loss = running_recon_loss / len(train_loader.dataset)\n","        epoch_kl_loss = running_kl_loss / len(train_loader.dataset)\n","        loss_history['train_loss'].append(epoch_loss)\n","        loss_history['recon_loss'].append(epoch_recon_loss)\n","        loss_history['kl_loss'].append(epoch_kl_loss)\n","        print(f\"Epoch: {epoch}, Loss: {epoch_loss}, Recon Loss: {epoch_recon_loss}, KL Loss: {epoch_kl_loss}\")\n","    \n","    return loss_history\n","\n","vae_loss_hist = train_vae(vae, vae_opt, train_loader, 200)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1676942984094,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"2bNv2K-HRkTW","outputId":"f6a2d9ee-a575-4269-a047-0f3a86736b10"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reconstruction loss: 2.0693\n"]}],"source":["# Calculate reconstruction loss on \"test\" subset\n","vae.eval()\n","test, test_lib = next(iter(DataLoader(train_dataset, batch_size=200, shuffle=False)))\n","test, test_lib = test.to(dev), test_lib.to(dev)\n","with torch.no_grad():\n","    x_hat, mu, log_var,= vae(test)\n","    x_hat = F.softmax(x_hat, dim=-1) * test_lib.unsqueeze(-1)\n","    loss = F.mse_loss(x_hat, test, reduction='mean')\n","    print(f\"Reconstruction loss: {loss.item():.4f}\")"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":578,"status":"ok","timestamp":1676942984665,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"dYRfZqCIUZG_","outputId":"aa6b9ff0-76c3-45e9-da51-ea22fa41e87f"},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","GMVAE                                    [4096, 2766]              --\n","├─Sequential: 1-1                        [4096, 20]                --\n","│    └─Linear: 2-1                       [4096, 512]               1,416,704\n","│    └─ReLU: 2-2                         [4096, 512]               --\n","│    └─Linear: 2-3                       [4096, 256]               131,328\n","│    └─ReLU: 2-4                         [4096, 256]               --\n","│    └─Linear: 2-5                       [4096, 20]                5,140\n","├─Sequential: 1-2                        [4096, 2766]              --\n","│    └─Linear: 2-6                       [4096, 256]               768\n","│    └─ReLU: 2-7                         [4096, 256]               --\n","│    └─Linear: 2-8                       [4096, 512]               131,584\n","│    └─ReLU: 2-9                         [4096, 512]               --\n","│    └─Linear: 2-10                      [4096, 2766]              1,418,958\n","==========================================================================================\n","Total params: 3,104,482\n","Trainable params: 3,104,482\n","Non-trainable params: 0\n","Total mult-adds (G): 12.72\n","==========================================================================================\n","Input size (MB): 45.32\n","Forward/backward pass size (MB): 141.62\n","Params size (MB): 12.42\n","Estimated Total Size (MB): 199.36\n","=========================================================================================="]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# ------------------------\n","# - Gaussian Mixture VAE -\n","# ------------------------\n","from torchinfo import summary\n","\n","class GMVAE(nn.Module):\n","    def __init__(self, input_dim, latent_dim, num_clusters):\n","        super(GMVAE, self).__init__()\n","        self.input_dim = input_dim\n","        self.latent_dim = latent_dim\n","        self.K = num_clusters\n","        \n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, latent_dim*num_clusters*2)\n","        )\n","        \n","        self.decoder = self.decode = nn.Sequential(\n","            nn.Linear(latent_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, input_dim)\n","        )\n","\n","    def encode(self, x):\n","        h = self.encoder(x)\n","        mu, log_var = torch.chunk(h, 2, dim=-1)\n","        mu = mu.view(-1, self.K, self.latent_dim)\n","        log_var = log_var.view(-1, self.K, self.latent_dim)\n","        return mu, log_var\n","\n","    def reparameterize(self, mu, log_var, temperature=1.0):\n","        eps = torch.randn_like(log_var)\n","        z = mu + torch.exp(log_var / 2) * eps\n","        # Gumbel-Softmax trick \n","        gumbel_softmax_logits = (z.view(-1, self.K, self.latent_dim) / temperature).softmax(dim=-1) # (B, K, latent_dim)\n","        z = (gumbel_softmax_logits * z.view(-1, self.K, self.latent_dim)).sum(dim=1) # (B, latent_dim)\n","        return z, gumbel_softmax_logits\n","\n","\n","    def forward(self, x, temperature=1.0):\n","        x = x.view(-1, self.input_dim)\n","        mu, log_var = self.encode(x)\n","        z, gumbel_softmax_logits = self.reparameterize(mu, log_var, temperature=temperature)\n","        x_hat = self.decode(z)\n","        return x_hat, mu, log_var, z, gumbel_softmax_logits\n","\n","dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","gm = GMVAE(input_dim=next(iter(train_loader))[0].shape[-1], latent_dim=2, num_clusters=5).to(dev)\n","gm_opt = optim.Adam(gm.parameters(), lr=1e-3, weight_decay=1e-5)\n","gm.train()\n","summary(gm, input_size=(2**12, 2766), device=dev)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1676942984665,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"0gOcwJH2awX3","outputId":"275da596-fd0c-4faa-cfb4-76fb2b6b9343"},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","VAE                                      [4096, 2766]              --\n","├─Sequential: 1-1                        [4096, 4]                 --\n","│    └─Linear: 2-1                       [4096, 512]               1,416,704\n","│    └─ReLU: 2-2                         [4096, 512]               --\n","│    └─Linear: 2-3                       [4096, 256]               131,328\n","│    └─ReLU: 2-4                         [4096, 256]               --\n","│    └─Linear: 2-5                       [4096, 4]                 1,028\n","├─Sequential: 1-2                        [4096, 2766]              --\n","│    └─Linear: 2-6                       [4096, 256]               768\n","│    └─ReLU: 2-7                         [4096, 256]               --\n","│    └─Linear: 2-8                       [4096, 512]               131,584\n","│    └─ReLU: 2-9                         [4096, 512]               --\n","│    └─Linear: 2-10                      [4096, 2766]              1,418,958\n","==========================================================================================\n","Total params: 3,100,370\n","Trainable params: 3,100,370\n","Non-trainable params: 0\n","Total mult-adds (G): 12.70\n","==========================================================================================\n","Input size (MB): 45.32\n","Forward/backward pass size (MB): 141.10\n","Params size (MB): 12.40\n","Estimated Total Size (MB): 198.82\n","=========================================================================================="]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["summary(vae, input_size=(2**12, 2766), device=dev)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556697,"status":"ok","timestamp":1676943541357,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"ESIx0tgqUbVY","outputId":"3462d5c2-f77e-40c8-b2a4-71238e445c49"},"outputs":[{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1/200 - Loss: 190505.3937 - Recon loss: 190494.0898 - KL loss: 11.3061\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 2/200 - Loss: 18970.9931 - Recon loss: 18970.9714 - KL loss: 0.0215\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 3/200 - Loss: 15684.9594 - Recon loss: 15684.9066 - KL loss: 0.0531\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 4/200 - Loss: 13172.8753 - Recon loss: 13172.7779 - KL loss: 0.0982\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 5/200 - Loss: 11709.4674 - Recon loss: 11709.3517 - KL loss: 0.1169\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 6/200 - Loss: 10418.4186 - Recon loss: 10418.2699 - KL loss: 0.1505\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 7/200 - Loss: 9736.9479 - Recon loss: 9736.7559 - KL loss: 0.1949\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 8/200 - Loss: 9408.1291 - Recon loss: 9407.9281 - KL loss: 0.2045\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 9/200 - Loss: 9234.8108 - Recon loss: 9234.6057 - KL loss: 0.2092\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 10/200 - Loss: 9022.9598 - Recon loss: 9022.7742 - KL loss: 0.1899\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 11/200 - Loss: 8784.8450 - Recon loss: 8784.6573 - KL loss: 0.1924\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 12/200 - Loss: 8640.4454 - Recon loss: 8640.2587 - KL loss: 0.1919\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 13/200 - Loss: 8593.5626 - Recon loss: 8593.3753 - KL loss: 0.1932\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 14/200 - Loss: 8448.4897 - Recon loss: 8448.2777 - KL loss: 0.2192\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 15/200 - Loss: 8173.6156 - Recon loss: 8173.3920 - KL loss: 0.2318\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 16/200 - Loss: 7995.9729 - Recon loss: 7995.7222 - KL loss: 0.2605\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 17/200 - Loss: 7841.0272 - Recon loss: 7840.7861 - KL loss: 0.2512\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 18/200 - Loss: 7605.7686 - Recon loss: 7605.5132 - KL loss: 0.2668\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 19/200 - Loss: 7516.9973 - Recon loss: 7516.7081 - KL loss: 0.3028\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 20/200 - Loss: 7432.7946 - Recon loss: 7432.5206 - KL loss: 0.2877\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 21/200 - Loss: 7663.9049 - Recon loss: 7663.5879 - KL loss: 0.3337\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 22/200 - Loss: 7795.2440 - Recon loss: 7795.0091 - KL loss: 0.2480\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 23/200 - Loss: 7653.9774 - Recon loss: 7653.7483 - KL loss: 0.2425\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 24/200 - Loss: 7603.9573 - Recon loss: 7603.6415 - KL loss: 0.3351\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 25/200 - Loss: 7367.8279 - Recon loss: 7367.4349 - KL loss: 0.4182\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 26/200 - Loss: 7137.2684 - Recon loss: 7136.8869 - KL loss: 0.4070\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 27/200 - Loss: 7045.5373 - Recon loss: 7045.1566 - KL loss: 0.4073\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 28/200 - Loss: 7026.6341 - Recon loss: 7026.2676 - KL loss: 0.3932\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 29/200 - Loss: 7001.5571 - Recon loss: 7001.1829 - KL loss: 0.4026\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 30/200 - Loss: 6953.6498 - Recon loss: 6953.2651 - KL loss: 0.4149\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 31/200 - Loss: 6900.6305 - Recon loss: 6900.2533 - KL loss: 0.4080\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 32/200 - Loss: 6799.8592 - Recon loss: 6799.4768 - KL loss: 0.4147\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 33/200 - Loss: 6786.5717 - Recon loss: 6786.1895 - KL loss: 0.4156\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 34/200 - Loss: 6735.2041 - Recon loss: 6734.7788 - KL loss: 0.4637\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 35/200 - Loss: 6651.1684 - Recon loss: 6650.7834 - KL loss: 0.4210\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 36/200 - Loss: 6591.6088 - Recon loss: 6591.2192 - KL loss: 0.4272\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 37/200 - Loss: 6515.8300 - Recon loss: 6515.4148 - KL loss: 0.4565\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 38/200 - Loss: 6416.6859 - Recon loss: 6416.2827 - KL loss: 0.4446\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 39/200 - Loss: 6314.1519 - Recon loss: 6313.7387 - KL loss: 0.4569\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 40/200 - Loss: 6225.3297 - Recon loss: 6224.8990 - KL loss: 0.4775\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 41/200 - Loss: 6078.5670 - Recon loss: 6078.1279 - KL loss: 0.4882\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 42/200 - Loss: 5951.7169 - Recon loss: 5951.2563 - KL loss: 0.5135\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 43/200 - Loss: 6008.8719 - Recon loss: 6008.4123 - KL loss: 0.5138\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 44/200 - Loss: 5997.2031 - Recon loss: 5996.7164 - KL loss: 0.5456\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 45/200 - Loss: 5896.6417 - Recon loss: 5896.1895 - KL loss: 0.5083\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 46/200 - Loss: 5865.7361 - Recon loss: 5865.3036 - KL loss: 0.4877\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 47/200 - Loss: 5793.1044 - Recon loss: 5792.6684 - KL loss: 0.4930\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 48/200 - Loss: 5767.0215 - Recon loss: 5766.5843 - KL loss: 0.4957\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 49/200 - Loss: 5745.4476 - Recon loss: 5744.9561 - KL loss: 0.5589\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 50/200 - Loss: 5719.0457 - Recon loss: 5718.5858 - KL loss: 0.5245\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 51/200 - Loss: 5748.7357 - Recon loss: 5748.2893 - KL loss: 0.5106\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 52/200 - Loss: 5861.0164 - Recon loss: 5860.5849 - KL loss: 0.4950\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 53/200 - Loss: 5708.7220 - Recon loss: 5708.2500 - KL loss: 0.5429\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 54/200 - Loss: 5702.4535 - Recon loss: 5702.0240 - KL loss: 0.4956\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 55/200 - Loss: 5624.0100 - Recon loss: 5623.5878 - KL loss: 0.4884\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 56/200 - Loss: 5626.8658 - Recon loss: 5626.4428 - KL loss: 0.4909\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 57/200 - Loss: 5613.9277 - Recon loss: 5613.4579 - KL loss: 0.5467\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 58/200 - Loss: 5567.5888 - Recon loss: 5567.1659 - KL loss: 0.4936\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 59/200 - Loss: 5541.4662 - Recon loss: 5541.0417 - KL loss: 0.4968\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 60/200 - Loss: 5507.0268 - Recon loss: 5506.6041 - KL loss: 0.4962\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 61/200 - Loss: 5475.7207 - Recon loss: 5475.2646 - KL loss: 0.5371\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 62/200 - Loss: 5443.4867 - Recon loss: 5443.0501 - KL loss: 0.5158\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 63/200 - Loss: 5421.6903 - Recon loss: 5421.2632 - KL loss: 0.5059\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 64/200 - Loss: 5496.6589 - Recon loss: 5496.2142 - KL loss: 0.5282\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 65/200 - Loss: 5402.0218 - Recon loss: 5401.6114 - KL loss: 0.4891\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 66/200 - Loss: 5390.9095 - Recon loss: 5390.4939 - KL loss: 0.4967\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 67/200 - Loss: 5380.4095 - Recon loss: 5379.9972 - KL loss: 0.4941\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 68/200 - Loss: 5341.5956 - Recon loss: 5341.1635 - KL loss: 0.5195\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 69/200 - Loss: 5934.7603 - Recon loss: 5934.3558 - KL loss: 0.4879\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 70/200 - Loss: 5469.6681 - Recon loss: 5469.2761 - KL loss: 0.4742\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 71/200 - Loss: 5353.3136 - Recon loss: 5352.9012 - KL loss: 0.5005\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 72/200 - Loss: 5226.5664 - Recon loss: 5226.1816 - KL loss: 0.4684\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 73/200 - Loss: 5176.7887 - Recon loss: 5176.4045 - KL loss: 0.4692\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 74/200 - Loss: 5131.0063 - Recon loss: 5130.6153 - KL loss: 0.4789\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 75/200 - Loss: 5114.4158 - Recon loss: 5114.0236 - KL loss: 0.4818\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 76/200 - Loss: 5084.8645 - Recon loss: 5084.4748 - KL loss: 0.4802\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 77/200 - Loss: 5057.3060 - Recon loss: 5056.9143 - KL loss: 0.4842\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 78/200 - Loss: 5063.5092 - Recon loss: 5063.1139 - KL loss: 0.4901\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 79/200 - Loss: 5049.4375 - Recon loss: 5049.0458 - KL loss: 0.4870\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 80/200 - Loss: 5018.7936 - Recon loss: 5018.3997 - KL loss: 0.4914\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 81/200 - Loss: 5034.4060 - Recon loss: 5033.8682 - KL loss: 0.6730\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 82/200 - Loss: 5000.8090 - Recon loss: 5000.4213 - KL loss: 0.4867\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 83/200 - Loss: 4999.4580 - Recon loss: 4999.0628 - KL loss: 0.4979\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 84/200 - Loss: 4980.6825 - Recon loss: 4980.2984 - KL loss: 0.4854\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 85/200 - Loss: 5022.5355 - Recon loss: 5022.1479 - KL loss: 0.4913\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 86/200 - Loss: 4988.3753 - Recon loss: 4987.9976 - KL loss: 0.4803\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 87/200 - Loss: 4959.5502 - Recon loss: 4959.1729 - KL loss: 0.4814\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 88/200 - Loss: 4957.9545 - Recon loss: 4957.5782 - KL loss: 0.4815\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 89/200 - Loss: 4968.3604 - Recon loss: 4967.8839 - KL loss: 0.6117\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 90/200 - Loss: 4932.7136 - Recon loss: 4932.3417 - KL loss: 0.4791\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 91/200 - Loss: 4892.3891 - Recon loss: 4892.0202 - KL loss: 0.4767\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 92/200 - Loss: 4904.5017 - Recon loss: 4904.1354 - KL loss: 0.4747\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 93/200 - Loss: 4899.0040 - Recon loss: 4898.6336 - KL loss: 0.4818\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 94/200 - Loss: 4891.8307 - Recon loss: 4891.3967 - KL loss: 0.5663\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 95/200 - Loss: 4863.5776 - Recon loss: 4863.2167 - KL loss: 0.4723\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 96/200 - Loss: 4895.9415 - Recon loss: 4895.5592 - KL loss: 0.5021\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 97/200 - Loss: 4875.9803 - Recon loss: 4875.6228 - KL loss: 0.4711\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 98/200 - Loss: 4868.0811 - Recon loss: 4867.7325 - KL loss: 0.4610\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 99/200 - Loss: 5038.0653 - Recon loss: 5037.7167 - KL loss: 0.4626\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 100/200 - Loss: 5116.1626 - Recon loss: 5115.8265 - KL loss: 0.4474\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 101/200 - Loss: 5129.6432 - Recon loss: 5129.3165 - KL loss: 0.4364\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 102/200 - Loss: 4985.9358 - Recon loss: 4985.6144 - KL loss: 0.4307\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 103/200 - Loss: 4887.6022 - Recon loss: 4887.2889 - KL loss: 0.4212\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 104/200 - Loss: 4975.3227 - Recon loss: 4974.9926 - KL loss: 0.4453\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 105/200 - Loss: 5097.6903 - Recon loss: 5097.3393 - KL loss: 0.4751\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 106/200 - Loss: 5794.8655 - Recon loss: 5794.2259 - KL loss: 0.8688\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 107/200 - Loss: 5623.7728 - Recon loss: 5623.1174 - KL loss: 0.8933\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 108/200 - Loss: 5104.2394 - Recon loss: 5103.5788 - KL loss: 0.9035\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 109/200 - Loss: 4895.8840 - Recon loss: 4895.2261 - KL loss: 0.9030\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 110/200 - Loss: 4837.3118 - Recon loss: 4836.6724 - KL loss: 0.8805\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 111/200 - Loss: 4811.1393 - Recon loss: 4810.5358 - KL loss: 0.8339\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 112/200 - Loss: 5047.7612 - Recon loss: 5047.1545 - KL loss: 0.8413\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 113/200 - Loss: 6272.8326 - Recon loss: 6271.9933 - KL loss: 1.1680\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 114/200 - Loss: 5354.6645 - Recon loss: 5353.8834 - KL loss: 1.0908\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 115/200 - Loss: 5039.2399 - Recon loss: 5038.4720 - KL loss: 1.0760\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 116/200 - Loss: 4842.3011 - Recon loss: 4841.5323 - KL loss: 1.0813\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 117/200 - Loss: 4758.8333 - Recon loss: 4758.0640 - KL loss: 1.0857\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 118/200 - Loss: 4798.6577 - Recon loss: 4797.8970 - KL loss: 1.0776\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 119/200 - Loss: 4722.3821 - Recon loss: 4721.6359 - KL loss: 1.0607\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 120/200 - Loss: 4680.8750 - Recon loss: 4680.1218 - KL loss: 1.0744\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 121/200 - Loss: 4669.8902 - Recon loss: 4669.1487 - KL loss: 1.0616\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 122/200 - Loss: 4632.7387 - Recon loss: 4632.0120 - KL loss: 1.0440\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 123/200 - Loss: 4626.6917 - Recon loss: 4625.9525 - KL loss: 1.0661\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 124/200 - Loss: 4614.2282 - Recon loss: 4613.5115 - KL loss: 1.0373\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 125/200 - Loss: 4594.0888 - Recon loss: 4593.2361 - KL loss: 1.2386\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 126/200 - Loss: 4590.2760 - Recon loss: 4589.5519 - KL loss: 1.0557\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 127/200 - Loss: 4585.7584 - Recon loss: 4584.6929 - KL loss: 1.5590\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 128/200 - Loss: 4572.0544 - Recon loss: 4570.8144 - KL loss: 1.8210\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 129/200 - Loss: 4565.0719 - Recon loss: 4564.3624 - KL loss: 1.0458\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 130/200 - Loss: 4569.6503 - Recon loss: 4568.9528 - KL loss: 1.0320\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 131/200 - Loss: 4554.8574 - Recon loss: 4554.1741 - KL loss: 1.0148\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 132/200 - Loss: 4559.0094 - Recon loss: 4558.3280 - KL loss: 1.0157\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 133/200 - Loss: 4549.8790 - Recon loss: 4549.2057 - KL loss: 1.0075\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 134/200 - Loss: 4541.1138 - Recon loss: 4540.4428 - KL loss: 1.0078\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 135/200 - Loss: 4525.3740 - Recon loss: 4524.7077 - KL loss: 1.0045\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 136/200 - Loss: 4511.0220 - Recon loss: 4510.3576 - KL loss: 1.0054\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 137/200 - Loss: 4515.4991 - Recon loss: 4514.8366 - KL loss: 1.0065\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 138/200 - Loss: 4504.7109 - Recon loss: 4504.0568 - KL loss: 0.9974\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 139/200 - Loss: 4485.7828 - Recon loss: 4485.0499 - KL loss: 1.1219\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 140/200 - Loss: 4472.9479 - Recon loss: 4472.2999 - KL loss: 0.9958\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 141/200 - Loss: 4484.2898 - Recon loss: 4483.6483 - KL loss: 0.9896\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 142/200 - Loss: 4475.2385 - Recon loss: 4474.5908 - KL loss: 1.0029\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 143/200 - Loss: 4488.6394 - Recon loss: 4488.0033 - KL loss: 0.9890\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 144/200 - Loss: 4503.0596 - Recon loss: 4502.4301 - KL loss: 0.9824\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 145/200 - Loss: 4467.5683 - Recon loss: 4466.9391 - KL loss: 0.9860\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 146/200 - Loss: 4481.5631 - Recon loss: 4480.9383 - KL loss: 0.9827\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 147/200 - Loss: 4472.7246 - Recon loss: 4472.1094 - KL loss: 0.9716\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 148/200 - Loss: 4498.3386 - Recon loss: 4497.7421 - KL loss: 0.9459\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 149/200 - Loss: 4432.7163 - Recon loss: 4432.1184 - KL loss: 0.9519\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 150/200 - Loss: 4385.6128 - Recon loss: 4385.0146 - KL loss: 0.9561\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 151/200 - Loss: 4371.9504 - Recon loss: 4371.3551 - KL loss: 0.9553\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 152/200 - Loss: 4357.6583 - Recon loss: 4357.0662 - KL loss: 0.9540\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 153/200 - Loss: 4365.7805 - Recon loss: 4365.1846 - KL loss: 0.9640\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 154/200 - Loss: 4365.6661 - Recon loss: 4365.0665 - KL loss: 0.9740\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 155/200 - Loss: 4343.1033 - Recon loss: 4342.5046 - KL loss: 0.9764\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 156/200 - Loss: 4355.2010 - Recon loss: 4354.6056 - KL loss: 0.9751\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 157/200 - Loss: 4357.2171 - Recon loss: 4356.6486 - KL loss: 0.9350\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 158/200 - Loss: 4371.6224 - Recon loss: 4371.0593 - KL loss: 0.9299\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 159/200 - Loss: 4367.4791 - Recon loss: 4366.9206 - KL loss: 0.9262\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 160/200 - Loss: 4343.9287 - Recon loss: 4343.3866 - KL loss: 0.9029\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 161/200 - Loss: 4315.5275 - Recon loss: 4314.9786 - KL loss: 0.9180\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 162/200 - Loss: 4312.4304 - Recon loss: 4311.8948 - KL loss: 0.8995\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 163/200 - Loss: 4304.1394 - Recon loss: 4303.6065 - KL loss: 0.8988\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 164/200 - Loss: 4275.8115 - Recon loss: 4275.2790 - KL loss: 0.9019\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 165/200 - Loss: 4251.5307 - Recon loss: 4250.8577 - KL loss: 1.1447\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 166/200 - Loss: 4261.5126 - Recon loss: 4260.9901 - KL loss: 0.8926\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 167/200 - Loss: 4248.6662 - Recon loss: 4248.1419 - KL loss: 0.8995\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 168/200 - Loss: 4265.0510 - Recon loss: 4264.5332 - KL loss: 0.8922\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 169/200 - Loss: 4325.9216 - Recon loss: 4325.4157 - KL loss: 0.8754\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 170/200 - Loss: 4298.5656 - Recon loss: 4298.0698 - KL loss: 0.8617\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 171/200 - Loss: 4281.5065 - Recon loss: 4281.0076 - KL loss: 0.8709\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 172/200 - Loss: 4211.5748 - Recon loss: 4211.0915 - KL loss: 0.8474\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 173/200 - Loss: 4293.2640 - Recon loss: 4292.7849 - KL loss: 0.8437\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 174/200 - Loss: 4300.1661 - Recon loss: 4299.6994 - KL loss: 0.8255\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 175/200 - Loss: 4365.9300 - Recon loss: 4365.4531 - KL loss: 0.8472\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 176/200 - Loss: 4574.6746 - Recon loss: 4574.2169 - KL loss: 0.8168\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 177/200 - Loss: 4391.4617 - Recon loss: 4391.0180 - KL loss: 0.7955\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 178/200 - Loss: 4232.2099 - Recon loss: 4231.7643 - KL loss: 0.8025\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 179/200 - Loss: 4214.8550 - Recon loss: 4214.4112 - KL loss: 0.8029\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 180/200 - Loss: 4207.8926 - Recon loss: 4207.4473 - KL loss: 0.8091\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 181/200 - Loss: 4265.3779 - Recon loss: 4264.9373 - KL loss: 0.8045\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 182/200 - Loss: 4222.0364 - Recon loss: 4221.6135 - KL loss: 0.7757\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 183/200 - Loss: 4212.0789 - Recon loss: 4211.6660 - KL loss: 0.7610\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 184/200 - Loss: 4307.6844 - Recon loss: 4307.2824 - KL loss: 0.7443\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 185/200 - Loss: 4199.1123 - Recon loss: 4198.7137 - KL loss: 0.7412\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 186/200 - Loss: 4150.9545 - Recon loss: 4150.5490 - KL loss: 0.7577\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 187/200 - Loss: 4130.4687 - Recon loss: 4130.0731 - KL loss: 0.7428\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 188/200 - Loss: 4128.5038 - Recon loss: 4128.1096 - KL loss: 0.7437\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 189/200 - Loss: 4109.5351 - Recon loss: 4109.0739 - KL loss: 0.8742\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 190/200 - Loss: 4114.9672 - Recon loss: 4114.5719 - KL loss: 0.7528\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 191/200 - Loss: 4116.3618 - Recon loss: 4115.9776 - KL loss: 0.7351\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 192/200 - Loss: 4092.9087 - Recon loss: 4092.5224 - KL loss: 0.7428\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 193/200 - Loss: 4139.2809 - Recon loss: 4138.8976 - KL loss: 0.7404\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 194/200 - Loss: 4098.3509 - Recon loss: 4097.9728 - KL loss: 0.7341\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 195/200 - Loss: 4065.2014 - Recon loss: 4064.8242 - KL loss: 0.7360\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 196/200 - Loss: 4064.2695 - Recon loss: 4063.8924 - KL loss: 0.7394\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 197/200 - Loss: 4094.6859 - Recon loss: 4094.3010 - KL loss: 0.7582\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 198/200 - Loss: 4099.8116 - Recon loss: 4099.4471 - KL loss: 0.7218\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 199/200 - Loss: 4063.4385 - Recon loss: 4063.0406 - KL loss: 0.7919\n"]},{"name":"stderr","output_type":"stream","text":["                                                                 "]},{"name":"stdout","output_type":"stream","text":["Epoch 200/200 - Loss: 4128.4287 - Recon loss: 4128.0235 - KL loss: 0.8105\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["import torch.optim as optim\n","from tqdm import tqdm\n","\n","def gmvae_loss(x, x_hat, mu, log_var, z, gumbel_softmax_logits, temperature=1.0):\n","    # Reconstruction loss\n","    recon_loss = F.mse_loss(x_hat, x, reduction='sum')\n","\n","    # KL divergence loss\n","    kl_div = (torch.exp(log_var) + mu**2 - 1 - log_var).sum(dim=-1)\n","    kl_loss = torch.mean(torch.sum(gumbel_softmax_logits * kl_div.unsqueeze(-1), dim=-1))\n","    \n","    # Total loss\n","    loss = recon_loss + temperature * kl_loss\n","    \n","    return loss, recon_loss, kl_loss\n","\n","\n","def train(model, opt, dataloader, num_epochs, lr=1e-3, weight_decay=1e-5, initial_temperature=1.0, final_temperature=0.5, device='cpu'): \n","    model.to(device)\n","    model.train()\n","    loss_history = {'train_loss': [], 'recon_loss': [], 'kl_loss': []}\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_recon_loss = 0.0\n","        running_kl_loss = 0.0\n","\n","        # Compute the temperature for this epoch based on the linear annealing schedule\n","        temperature = max(initial_temperature - epoch * (initial_temperature - final_temperature) / (num_epochs - 1), final_temperature)\n","        for x, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\", leave=False):\n","            x = x.to(device)\n","            opt.zero_grad()\n","            x_hat, mu, log_var, z, gumbel_softmax_logits = model(x, temperature=temperature)\n","            loss, recon_loss, kl_loss = gmvae_loss(x, x_hat, mu, log_var, z, gumbel_softmax_logits, temperature=temperature)\n","            loss.backward()\n","            opt.step()\n","            running_loss += loss.item()\n","            running_recon_loss += recon_loss.item()\n","            running_kl_loss += kl_loss.item()\n","\n","        epoch_loss = running_loss / len(dataloader.dataset)\n","        epoch_recon_loss = running_recon_loss / len(dataloader.dataset)\n","        epoch_kl_loss = running_kl_loss / len(dataloader.dataset)\n","\n","        loss_history['train_loss'].append(epoch_loss)\n","        loss_history['recon_loss'].append(epoch_recon_loss)\n","        loss_history['kl_loss'].append(epoch_kl_loss)\n","        tqdm.write(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Recon loss: {epoch_recon_loss:.4f} - KL loss: {epoch_kl_loss:.4f}\")\n","    model.eval()\n","    \n","    return loss_history\n","\n","gm_loss_hist = train(gm, gm_opt, train_loader, num_epochs=200, initial_temperature=1.0, final_temperature=0.5, device=dev)\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1676943541359,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"j-VNXh9jUivq","outputId":"7f6650c7-7551-4dc9-e781-36bcd3445474"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reconstruction loss: 1.2692\n"]}],"source":["# Calculate reconstruction loss on the complete dataset\n","gm.eval()\n","\n","with torch.no_grad():\n","    x_hat, mu, log_var, z, gumbel_softmax_logits = gm(test, temperature=0.5)\n","    loss = F.mse_loss(x_hat, test, reduction='mean')\n","    print(f\"Reconstruction loss: {loss.item():.4f}\")"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1676943541359,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"CcVnHkBoXnGa","outputId":"5ef67617-29e9-4194-943a-9281a55b9943"},"outputs":[{"data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7fc780a69460>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["from matplotlib import pyplot as plt\n","\n","zt = z.cpu().detach().numpy()\n","\n","plt.scatter(zt[:,0], zt[:,1])"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":922,"status":"ok","timestamp":1676944166382,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"xoXK_Kwof_x_"},"outputs":[],"source":["from datetime import datetime\n","import os\n","\n","# save model and include timestamp\n","def save_model(model, optimizer, loss_hist, path):\n","    if not os.path.exists('checkpoints'):\n","        os.makedirs('checkpoints')\n","        \n","    full_path = f\"checkpoints/{path}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\"\n","    torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss_hist': loss_hist\n","    }, full_path)\n","    print(f\"Model saved to {full_path}.\")\n","\n","def load_model(path, model_type, model_params, dev):\n","    # check if path is a full path or just a filename\n","    if not os.path.exists(path):\n","        path = f\"checkpoints/{path}\"\n","    checkpoint = torch.load(path)\n","    # create model based on model_type\n","    if model_type == \"gmvae\":\n","        model = GMVAE(**model_params).to(dev)\n","    elif model_type == \"vae\":\n","        model = VAE(**model_params).to(dev)\n","    else:\n","        raise ValueError(\"Model type not recognized.\")   \n","\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    loss_hist = checkpoint['loss_hist']\n","    return model, optimizer, loss_hist\n","\n","# save_model(gm, gm_opt, gm_loss_hist, path=\"gmvae\")\n","# save_model(vae, vae_opt, vae_loss_hist, path=\"vae\")\n","\n","# gm2, gm_opt, gm_loss_hist2 = load_model(\"gmvae_20230221_014308.pt\", \"gmvae\", {\"input_dim\": 2766, \"latent_dim\": 2, \"num_clusters\": 5}, dev)\n","# vae2, vae_opt, vae_loss_hist2 = load_model(\"vae_20230221_014308.pt\", \"vae\", {\"input_dim\": 2766, \"latent_dim\": 2}, dev)"]},{"cell_type":"markdown","metadata":{"id":"EoUhgJvfsmgt"},"source":["## Gene \"correlation\" matrix C"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":490,"status":"ok","timestamp":1676944249607,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"B_FwvDkfUpaO"},"outputs":[],"source":["from functorch import jacfwd, vmap\n","\n","def jac(f, z):\n","    # composed with vmap for batched Jacobians\n","    return vmap(jacfwd(f))(z)\n","    \n","def jac_robust(f, z):\n","    # alternative jac if experiencing crashes \n","    batch_size, z_dim = z.size()\n","    v = torch.eye(z_dim).unsqueeze(0).repeat(batch_size, 1, 1).view(-1, z_dim).to(z)\n","    z = z.repeat(1, z_dim).view(-1, z_dim)\n","    return torch.autograd.functional.jvp(f, z, v=v)[1].view(batch_size, z_dim, -1).permute(0, 2, 1)\n","\n","# gene \"correlation\" matrix C\n","def C_matrix(f, z, normalize=False):\n","    '''\n","    f:      model.decode: (b, m) -> (b, n) \n","    z:      torch.tensor whose size = (b, m) (keep b low for memory)\n","    out:    torch.tensor whose size = (b, n, n)\n","    '''\n","    J = jac(f, z)\n","\n","    # J @ J.T on batch \n","    if normalize:\n","        return torch.bmm(J, J.transpose(1,2)) / J.norm(p=2, dim=2).unsqueeze(-1)\n","    else:\n","        return torch.bmm(J, J.transpose(1,2))\n","\n","# jacrev: 1.45s\n","# jacfwd: 80-90ms \n","# jvp:    85-92ms "]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1676944249122,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"mbm0g5fKUo0T"},"outputs":[],"source":["# directly on gradients \n","\n","# z = model.reparameterize(*model.encoder(test))\n","# for i in range(z.shape[1]):\n","#     model.decoder.zero_grad()\n","#     leaf = z[0].detach().requires_grad_()\n","#     out = model.decoder(z[0])[i]\n","#     out.retain_grad() \n","#     out.backward(retain_graph=True)\n","#     gene_gradients.append(out.grad.clone().detach())\n","# G = torch.stack(gene_gradients)\n"]},{"cell_type":"markdown","metadata":{"id":"bbWh1dYGdaST"},"source":["# Geodesics"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1676944251001,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"AWImcXz03udq"},"outputs":[],"source":["def get_Riemannian_metric(f, z): #J.T @ J instead! \n","    '''\n","    f:      model.decode: function (b, m) -> (b, n)\n","    z:      torch.tensor whose size = (b, m)\n","    out:    torch.tensor whose size = (b, n, n)\n","    Outputs the Riemannian metric on the manifold of gene expression vectors\n","    '''\n","    J = jac_robust(f, z)\n","    out = torch.einsum('nij,nik->njk', J, J)\n","    return out"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676944251387,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"VePXtTJkdcvC"},"outputs":[],"source":["def compute_length_of_curve(curve, pretrained_model, get_Riemannian_metric):\n","    '''\n","    curve:  torch.tensor whose size = (L, d)\n","    out:    torch.tensor whose size = (1)\n","    '''\n","    dt = 1/(len(curve)-1)\n","    velocity = (curve[:-1] - curve[1:])/dt # (L-1, d)\n","    G = get_Riemannian_metric(pretrained_model.decode, curve[:-1]) # (L-1, d, d)\n","    out = torch.sqrt(torch.einsum('ni, nij, nj -> n', velocity, G, velocity)).sum() * dt # int(sqrt(velocity.T @ G @ velocity)) dt\n","    return out\n"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676944517277,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"AfpwoinQoM0L","outputId":"4f3de32d-3bc2-42a6-9f49-14e764996744"},"outputs":[{"name":"stdout","output_type":"stream","text":["  VAE: 336.583252, 456.003662\n","GMVAE: 162.408325, 130.855530\n"]}],"source":["# compute the length of two crossing curves in latent space ( X )\n","curve1 = torch.tensor([[-2, -2], [-1.5, -1.5], [-1, -1], [-0.5, -0.5], [0, 0], [0.5, 0.5], [1, 1], [1.5, 1.5], [2, 2]]).to(dev)\n","curve2 = torch.tensor([[-2, 2], [-1.5, 1.5], [-1, 1], [-0.5, 0.5], [0, 0], [0.5, -0.5], [1, -1], [1.5, -1.5], [2, -2]]).to(dev)\n","\n","vae_len1 = compute_length_of_curve(curve1, vae, get_Riemannian_metric).item()\n","vae_len2 = compute_length_of_curve(curve2, vae, get_Riemannian_metric).item()\n","\n","gm_len1 = compute_length_of_curve(curve1, gm, get_Riemannian_metric).item()\n","gm_len2 = compute_length_of_curve(curve2, gm, get_Riemannian_metric).item()\n","\n","print(\"  VAE: %f, %f\" % (vae_len1, vae_len2))\n","print(\"GMVAE: %f, %f\" % (gm_len1, gm_len2))"]},{"cell_type":"markdown","metadata":{"id":"KnLRNmwhsPi-"},"source":["### BFGS optimizer to minimize geodesic "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1676943541769,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"yWsB3ScDdbZM"},"outputs":[],"source":["def compute_geodesic(z1, z2, pretrained_model, get_Riemannian_metric, num_discretization=100):\n","    '''\n","    z1 : torch.tensor whose size = (1, 2)\n","    z1 : torch.tensor whose size = (1, 2)\n","    out: torch.tensor whose size = (num_discretization, 2)\n","    '''\n","    from scipy.optimize import minimize\n","    class GeodesicFittingTool():\n","        def __init__(self, z1, z2, z_init, pretrained_model, get_Riemannian_metric, num_discretization, method, device=f\"cuda:{0}\"):\n","            self.z1 = z1\n","            self.z2 = z2\n","            self.pretrained_model = pretrained_model\n","            self.get_Riemannian_metric = get_Riemannian_metric\n","            self.num_discretization = num_discretization\n","            self.delta_t = 1/(num_discretization-1)\n","            self.device = device\n","            self.method = method\n","            self.z_init_input = z_init\n","            self.initialize()\n","            \n","        def initialize(self):\n","            self.z_init= self.z1.squeeze(0)\n","            self.z_final= self.z2.squeeze(0)\n","            dim = self.z_final.size(0)\n","            self.init_z = self.z_init_input.detach().cpu().numpy()\n","            self.z_shape = self.init_z.shape\n","            self.init_z_vec = self.init_z.flatten()\n","\n","        def geodesic_loss(self, z): \n","            z_torch = torch.tensor(z.reshape(self.z_shape), dtype=torch.float32).to(self.device)\n","            z_extended = torch.cat([self.z_init.unsqueeze(0), z_torch, self.z_final.unsqueeze(0)], dim=0)\n","            G_ = self.get_Riemannian_metric(self.pretrained_model.decode, z_extended[:-1])\n","            delta_z = (z_extended[1:, :]-z_extended[:-1, :])/(self.delta_t)\n","            loss = torch.einsum('ni, nij, nj -> ', delta_z, G_, delta_z) * self.delta_t\n","            return loss.item()\n","        \n","        def jac(self, z):\n","            z_torch = torch.tensor(z.reshape(self.z_shape), dtype=torch.float32).to(self.device)\n","            z_torch.requires_grad = True\n","            z_extended = torch.cat([self.z_init.unsqueeze(0), z_torch, self.z_final.unsqueeze(0)], dim=0)\n","            G_ = self.get_Riemannian_metric(self.pretrained_model.decode, z_extended[:-1], create_graph=True)\n","            delta_z = (z_extended[1:, :]-z_extended[:-1, :])/(self.delta_t)\n","            loss = torch.einsum('ni, nij, nj -> ', delta_z, G_, delta_z) * self.delta_t\n","            loss.backward()\n","            z_grad = z_torch.grad\n","            return z_grad.detach().cpu().numpy().flatten()\n","\n","        def callback(self, z):\n","            self.Nfeval += 1\n","            return print('{} th loss : {}'.format(self.Nfeval, self.geodesic_loss(z)))\n","            \n","        def BFGS_optimizer(self, callback=False, maxiter=1000):\n","            self.Nfeval = 0\n","            z0 = self.init_z_vec\n","            if callback == True:\n","                call = self.callback\n","            else:\n","                call = None\n","            res = minimize(\n","                self.geodesic_loss, \n","                z0, \n","                callback=call, \n","                method=self.method,\n","                jac = self.jac,\n","                options = {\n","                    'gtol': 1e-10, \n","                    'eps': 1.4901161193847656e-08, \n","                    'maxiter': maxiter, \n","                    'disp': True, \n","                    'return_all': False, \n","                    'finite_diff_rel_step': None}\n","                )\n","            self.res = res\n","\n","    z12_linear_curve = torch.cat([z1.to(device) + (z2.to(device) - z1.to(device)) * t/(num_discretization-1) for t in range(num_discretization)], dim=0)\n","    \n","    tool = GeodesicFittingTool(z1, z2, z12_linear_curve[1:-1], pretrained_model, get_Riemannian_metric, num_discretization, 'BFGS', device=device)\n","    tool.BFGS_optimizer()\n","    z_torch = torch.tensor(tool.res['x'].reshape(tool.z_shape), dtype=torch.float32).to(device)\n","    out = torch.cat([tool.z_init.unsqueeze(0), z_torch, tool.z_final.unsqueeze(0)], dim=0)\n","    return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1676943541770,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"3TbwaJy63ktw"},"outputs":[],"source":["z1 = torch.tensor([[-1, 1]], dtype=torch.float32) \n","z2 = torch.tensor([[0.6, -1.2]], dtype=torch.float32) \n","\n","num_discretization = 100\n","z12_geodesic_curve = compute_geodesic(\n","    z1.to(device), \n","    z2.to(device), \n","    pretrained_model, \n","    get_Riemannian_metric, \n","    num_discretization=num_discretization\n",")\n","assert z12_geodesic_curve.size() == torch.Size([num_discretization, 2])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1676943541771,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"IOkivSru5FLc"},"outputs":[],"source":["L12 = compute_length_of_curve(\n","    z12_geodesic_curve, pretrained_model, get_Riemannian_metric)\n","L12\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"aborted","timestamp":1676943541772,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"XQ3qhCoe5zF4"},"outputs":[],"source":["z12_linear_curve = torch.cat([z1.to(device) + (z2.to(device) - z1.to(device)) * t/(num_discretization-1) for t in range(num_discretization)], dim=0)\n","\n","latent_embeddings = pretrained_model.encode(\n","    train_ds.data.to(device)\n","    ).detach().cpu().numpy()\n","\n","z_scale = np.minimum(\n","    np.max(latent_embeddings, axis=0), \n","    np.min(latent_embeddings, axis=0)\n",")\n","labels = torch.unique(train_ds.targets)\n","\n","f = plt.figure(figsize=(7, 7))\n","plt.title('Latent space embeddings')\n","for label in labels:\n","    classwise_le = latent_embeddings[train_ds.targets == label]\n","    plt.scatter(\n","        classwise_le[:200, 0], \n","        classwise_le[:200, 1], \n","        label=label.item(), s=5)\n","\n","plt.scatter(z1[0, 0], z1[0, 1], c='tab:red', marker='s')\n","plt.scatter(z2[0, 0], z2[0, 1], c='tab:green', marker='s')\n","\n","plt.scatter(z12_linear_curve[:, 0].detach().cpu(), z12_linear_curve[:, 1].detach().cpu(), s=10, c='tab:gray')\n","plt.scatter(z12_geodesic_curve[:, 0].detach().cpu(), z12_geodesic_curve[:, 1].detach().cpu(), s=10, c='tab:pink')\n","\n","plt.xlim(-3, 3)\n","plt.ylim(-3, 3)\n","\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":27,"status":"aborted","timestamp":1676943541773,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"eJOoNKJr5zrN"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPH707U6ApZYOlncpE4IGdJ","mount_file_id":"1gEzMrTlr7G_N89-yMbnXE_sKWSHFhCj9","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
