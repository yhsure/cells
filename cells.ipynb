{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1676637570442,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"oSbu8jTsZh8s","outputId":"4db5a8a6-52cf-463a-8f56-79f543867884"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/cells\n"]}],"source":["cd /content/drive/MyDrive/Colab Notebooks/cells"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676637570443,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"0GB9rFhVYcsW"},"outputs":[],"source":["!pip install torchinfo"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12715,"status":"ok","timestamp":1676637583154,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"NC2-tZ5QZsmw","outputId":"f52a36d0-8421-4ca8-9570-2f408a342721"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([86024, 2766])\n"]}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.io import mmread\n","\n","\n","class RNA(Dataset):\n","    def __init__(self, data_file):\n","        # load the scRNA-seq data from the specified file\n","        self.data = torch.from_numpy(\n","            mmread(data_file).astype(\"float32\").transpose().todense())\n","        print(self.data.shape)\n","        \n","    def __len__(self):\n","        # return the number of examples in the dataset\n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        # return the preprocessed data for the specified example\n","        library = self.data[index].sum(dim=-1)\n","        example = self.data[index]\n","        return example, library\n","\n","        \n","\n","# datasets: hcl, celegan, uc_epi, zfish_ep50_5226\n","# train_dataset = RNA(\"cells/data/hcl.mtx\")\n","# train_dataset = RNA(\"data/zfish_ep50_5226.mtx\")\n","train_dataset = RNA(\"cells/data/celegan.mtx\")\n","train_loader = DataLoader(train_dataset, batch_size=2**12, shuffle=True) # 2**15\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2466,"status":"ok","timestamp":1676637585597,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"oeKzvTc0Z1Il"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","\n","# ---------------------------\n","# - Variational Autoencoder -\n","# ---------------------------\n","class VAE(nn.Module):\n","    def __init__(self, input_dim, latent_dim):\n","        super(VAE, self).__init__()\n","        self.input_dim = input_dim\n","        self.latent_dim = latent_dim\n","        \n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, latent_dim * 2) # mu + log_var\n","        )\n","        \n","        self.decoder = self.decode = nn.Sequential(\n","            nn.Linear(latent_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, input_dim))\n","        \n","    def encode(self, x):\n","        h = self.encoder(x)\n","        mu, log_var = torch.chunk(h, 2, dim=-1)\n","        return mu, log_var\n","    \n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(0.5 * log_var)\n","        eps = torch.randn_like(std)\n","        z = mu + eps * std\n","        return z\n","    \n","    def forward(self, x):\n","        mu, log_var = self.encode(x)\n","        z = self.reparameterize(mu, log_var)\n","        x_hat = self.decode(z)\n","        return x_hat, mu, log_var\n","\n","    def loss(self, x, x_hat, mu, log_var):\n","        recon_loss = F.mse_loss(x_hat, x, reduction=\"sum\")\n","        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","        return recon_loss, kl_div\n","\n","\n","dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model = VAE(input_dim=next(iter(train_loader))[0].shape[-1], latent_dim=2).to(dev)\n","model.train()\n","opt = optim.Adam(model.parameters())"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":280331,"status":"ok","timestamp":1676637865925,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"V9WqlbuKaG_-","outputId":"df95b047-fbd8-4b40-9c3a-4e71569cb803"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss: 26612.05279678945, Recon Loss: 26609.717068283633, KL Loss: 2.3359475774032443\n","Epoch: 1, Loss: 24289.28687383935, Recon Loss: 24287.39751960572, KL Loss: 1.889096815985413\n","Epoch: 2, Loss: 23380.654631129466, Recon Loss: 23379.228502865117, KL Loss: 1.4260615599573374\n","Epoch: 3, Loss: 22323.501555525203, Recon Loss: 22322.871208906818, KL Loss: 0.630262926524377\n","Epoch: 4, Loss: 21942.881872378093, Recon Loss: 21941.891546209124, KL Loss: 0.990250830477164\n","Epoch: 5, Loss: 21119.356861847784, Recon Loss: 21117.853136642825, KL Loss: 1.5036564952859566\n","Epoch: 6, Loss: 19284.66636817923, Recon Loss: 19282.382269228645, KL Loss: 2.284171900726591\n","Epoch: 7, Loss: 20220.290732410067, Recon Loss: 20217.85750011443, KL Loss: 2.4330699756446355\n","Epoch: 8, Loss: 18962.450044963833, Recon Loss: 18959.526454690335, KL Loss: 2.923549026530764\n","Epoch: 9, Loss: 20031.172437978792, Recon Loss: 20027.966939095484, KL Loss: 3.2055574993771043\n","Epoch: 10, Loss: 20437.41546245452, Recon Loss: 20433.901536744106, KL Loss: 3.513953863205294\n","Epoch: 11, Loss: 17365.667827165238, Recon Loss: 17361.409234995466, KL Loss: 4.2586863229610215\n","Epoch: 12, Loss: 19060.975727340625, Recon Loss: 19055.836495120548, KL Loss: 5.139274614684074\n","Epoch: 13, Loss: 15751.403981215453, Recon Loss: 15744.795466372394, KL Loss: 6.608454520254703\n","Epoch: 14, Loss: 14564.547905950938, Recon Loss: 14556.417116732595, KL Loss: 8.130639665705276\n","Epoch: 15, Loss: 14514.414680605121, Recon Loss: 14505.469589745668, KL Loss: 8.945020712199105\n","Epoch: 16, Loss: 16253.314129632428, Recon Loss: 16244.523815937267, KL Loss: 8.790409654326233\n","Epoch: 17, Loss: 18166.628921779822, Recon Loss: 18157.703418250283, KL Loss: 8.925522985629543\n","Epoch: 18, Loss: 17184.765305503406, Recon Loss: 17175.501941863607, KL Loss: 9.263358649526387\n","Epoch: 19, Loss: 14888.702261596327, Recon Loss: 14878.751914255034, KL Loss: 9.950134145152587\n","Epoch: 20, Loss: 14560.962246954701, Recon Loss: 14549.902015353273, KL Loss: 11.060200019491603\n","Epoch: 21, Loss: 16664.987296877178, Recon Loss: 16653.453067455648, KL Loss: 11.534205084433282\n","Epoch: 22, Loss: 16461.278916893833, Recon Loss: 16450.211229160974, KL Loss: 11.067691667949301\n","Epoch: 23, Loss: 14757.40042422754, Recon Loss: 14746.39024978676, KL Loss: 11.01011657998649\n","Epoch: 24, Loss: 15201.134669176625, Recon Loss: 15190.473966385835, KL Loss: 10.66069403921273\n","Epoch: 25, Loss: 15639.903632702612, Recon Loss: 15629.521288776425, KL Loss: 10.382330982804598\n","Epoch: 26, Loss: 15274.328634101203, Recon Loss: 15262.954035969431, KL Loss: 11.3746638021768\n","Epoch: 27, Loss: 14770.506100268893, Recon Loss: 14758.773323319474, KL Loss: 11.732838615333048\n","Epoch: 28, Loss: 13925.148716832475, Recon Loss: 13914.20160230539, KL Loss: 10.947230367076061\n","Epoch: 29, Loss: 12207.191555168289, Recon Loss: 12193.26792888558, KL Loss: 13.923681856387939\n","Epoch: 30, Loss: 10951.509923783988, Recon Loss: 10931.242471623109, KL Loss: 20.26740484522363\n","Epoch: 31, Loss: 11991.031302560732, Recon Loss: 11973.564799412808, KL Loss: 17.466503918453625\n","Epoch: 32, Loss: 11537.42552260125, Recon Loss: 11520.48403933786, KL Loss: 16.941422388585327\n","Epoch: 33, Loss: 12327.436031614361, Recon Loss: 12312.37985929072, KL Loss: 15.05624231187552\n","Epoch: 34, Loss: 10776.692024877324, Recon Loss: 10759.809953880966, KL Loss: 16.88212161657035\n","Epoch: 35, Loss: 10101.023315304814, Recon Loss: 10081.686178511956, KL Loss: 19.337162761924144\n","Epoch: 36, Loss: 9411.717930188814, Recon Loss: 9391.530259905667, KL Loss: 20.187689083116748\n","Epoch: 37, Loss: 9576.381738626356, Recon Loss: 9556.154347982649, KL Loss: 20.22737575670425\n","Epoch: 38, Loss: 9166.453940270303, Recon Loss: 9146.221651715656, KL Loss: 20.232217260641153\n","Epoch: 39, Loss: 8865.076334438645, Recon Loss: 8844.1757827939, KL Loss: 20.900546416697097\n","Epoch: 40, Loss: 8604.223313225088, Recon Loss: 8583.052486806007, KL Loss: 21.1708260893345\n","Epoch: 41, Loss: 8426.839737402499, Recon Loss: 8405.411940386004, KL Loss: 21.42781835864846\n","Epoch: 42, Loss: 8505.484070193166, Recon Loss: 8483.20708678501, KL Loss: 22.27696461253174\n","Epoch: 43, Loss: 8067.190844451839, Recon Loss: 8044.826642401394, KL Loss: 22.364172794558797\n","Epoch: 44, Loss: 7979.860599858034, Recon Loss: 7956.395506314008, KL Loss: 23.465113167739254\n","Epoch: 45, Loss: 7710.912378286147, Recon Loss: 7687.256807693202, KL Loss: 23.65554152419265\n","Epoch: 46, Loss: 7602.198187492553, Recon Loss: 7579.372855862717, KL Loss: 22.825332449500888\n","Epoch: 47, Loss: 7483.7207873583975, Recon Loss: 7460.888874216788, KL Loss: 22.83181779120541\n","Epoch: 48, Loss: 7325.23277088734, Recon Loss: 7301.30297967355, KL Loss: 23.92978482746162\n","Epoch: 49, Loss: 7146.012352089788, Recon Loss: 7122.213593576538, KL Loss: 23.798760060519935\n","Epoch: 50, Loss: 7072.803372641646, Recon Loss: 7047.872824280869, KL Loss: 24.930549097961354\n","Epoch: 51, Loss: 7122.525827094764, Recon Loss: 7096.970247946794, KL Loss: 25.555585126327845\n","Epoch: 52, Loss: 7032.929278230131, Recon Loss: 7007.86472184543, KL Loss: 25.064568022667036\n","Epoch: 53, Loss: 6753.6001689935365, Recon Loss: 6728.5978449691365, KL Loss: 25.002319555177053\n","Epoch: 54, Loss: 6684.464199285047, Recon Loss: 6659.869655402308, KL Loss: 24.59445569930515\n","Epoch: 55, Loss: 6724.543970832267, Recon Loss: 6699.533098525739, KL Loss: 25.010910180694147\n","Epoch: 56, Loss: 6607.479861974383, Recon Loss: 6580.686421449247, KL Loss: 26.793430889594013\n","Epoch: 57, Loss: 6860.733762979663, Recon Loss: 6833.178092559925, KL Loss: 27.5556494319848\n","Epoch: 58, Loss: 6977.431945420482, Recon Loss: 6951.679602698992, KL Loss: 25.75231902268746\n","Epoch: 59, Loss: 6641.694861703747, Recon Loss: 6614.5375508397365, KL Loss: 27.157338290062437\n","Epoch: 60, Loss: 6514.168635124718, Recon Loss: 6485.8996406072, KL Loss: 28.268966437827306\n","Epoch: 61, Loss: 6198.827025561181, Recon Loss: 6171.120010022639, KL Loss: 27.707088286695086\n","Epoch: 62, Loss: 6130.037925882693, Recon Loss: 6101.944667603663, KL Loss: 28.093226581886736\n","Epoch: 63, Loss: 6241.639415808016, Recon Loss: 6212.561350339004, KL Loss: 29.078047861379325\n","Epoch: 64, Loss: 6360.372235646084, Recon Loss: 6331.3093249231315, KL Loss: 29.062930255138873\n","Epoch: 65, Loss: 6491.1111794564595, Recon Loss: 6462.273620338692, KL Loss: 28.83752041861677\n","Epoch: 66, Loss: 6491.2794708362635, Recon Loss: 6461.83007848827, KL Loss: 29.449384175998315\n","Epoch: 67, Loss: 6670.152886953241, Recon Loss: 6641.606100841044, KL Loss: 28.54673717086907\n","Epoch: 68, Loss: 6144.02357111551, Recon Loss: 6113.812107258948, KL Loss: 30.2114675822152\n","Epoch: 69, Loss: 6056.519807290335, Recon Loss: 6026.106805518119, KL Loss: 30.412984875338715\n","Epoch: 70, Loss: 6081.736897256688, Recon Loss: 6052.111610749616, KL Loss: 29.625280335017564\n","Epoch: 71, Loss: 6323.546352276146, Recon Loss: 6294.016520825584, KL Loss: 29.52981418828504\n","Epoch: 72, Loss: 6441.362762942155, Recon Loss: 6411.718935835675, KL Loss: 29.643818293617326\n","Epoch: 73, Loss: 6109.3748263565985, Recon Loss: 6080.0616040835985, KL Loss: 29.31322719888289\n","Epoch: 74, Loss: 5846.45534926118, Recon Loss: 5816.583170694007, KL Loss: 29.872141336891403\n","Epoch: 75, Loss: 5691.576221189, Recon Loss: 5660.607418605564, KL Loss: 30.968769068342358\n","Epoch: 76, Loss: 5598.484715566296, Recon Loss: 5566.453141892088, KL Loss: 32.03159241954371\n","Epoch: 77, Loss: 5601.003807008981, Recon Loss: 5569.334017478044, KL Loss: 31.669758205933537\n","Epoch: 78, Loss: 5573.725911723218, Recon Loss: 5542.081932326843, KL Loss: 31.643916368551015\n","Epoch: 79, Loss: 5637.615472748703, Recon Loss: 5607.434752719808, KL Loss: 30.180672890971696\n","Epoch: 80, Loss: 5535.569142053108, Recon Loss: 5505.642834174093, KL Loss: 29.926334309797447\n","Epoch: 81, Loss: 5424.729738893724, Recon Loss: 5394.032509550387, KL Loss: 30.697222626908793\n","Epoch: 82, Loss: 5465.73688996857, Recon Loss: 5434.696119415076, KL Loss: 31.04077145457545\n","Epoch: 83, Loss: 5486.085208347561, Recon Loss: 5454.449065540576, KL Loss: 31.63620410980562\n","Epoch: 84, Loss: 5324.285579006295, Recon Loss: 5292.180360004912, KL Loss: 32.10525243364134\n","Epoch: 85, Loss: 5392.1003308533445, Recon Loss: 5359.12899877051, KL Loss: 32.97134398243366\n","Epoch: 86, Loss: 5251.028627321663, Recon Loss: 5217.316813067792, KL Loss: 33.71184477215865\n","Epoch: 87, Loss: 5305.977988337557, Recon Loss: 5273.310140102617, KL Loss: 32.667812351846564\n","Epoch: 88, Loss: 5377.379265524737, Recon Loss: 5346.5619867211835, KL Loss: 30.81729650679249\n","Epoch: 89, Loss: 5418.241045694734, Recon Loss: 5388.991688093584, KL Loss: 29.249396733990228\n","Epoch: 90, Loss: 5316.755779693839, Recon Loss: 5285.924428282278, KL Loss: 30.83133696942155\n","Epoch: 91, Loss: 5840.999031883544, Recon Loss: 5810.967527957314, KL Loss: 30.031543694617394\n","Epoch: 92, Loss: 5755.6756999681775, Recon Loss: 5727.879704083308, KL Loss: 27.795997858734825\n","Epoch: 93, Loss: 5432.482743732854, Recon Loss: 5405.124201258515, KL Loss: 27.358491172637983\n","Epoch: 94, Loss: 5178.377871882048, Recon Loss: 5149.287577449317, KL Loss: 29.09029679008831\n","Epoch: 95, Loss: 5101.882356413617, Recon Loss: 5072.006678368827, KL Loss: 29.875704473976\n","Epoch: 96, Loss: 5138.710627187344, Recon Loss: 5109.089339076159, KL Loss: 29.621288109056277\n","Epoch: 97, Loss: 5079.308873332224, Recon Loss: 5049.770310738137, KL Loss: 29.53853081730059\n","Epoch: 98, Loss: 4995.883341013482, Recon Loss: 4964.238943356288, KL Loss: 31.644391644780363\n","Epoch: 99, Loss: 4954.763422092326, Recon Loss: 4922.254493125192, KL Loss: 32.508965299527034\n"]}],"source":["def train_vae(model, train_loader, num_epochs=150, kl_weight=4):\n","    kl_weight = 1\n","\n","    loss_history = {'train_loss': [], 'recon_loss': [], 'kl_loss': []}\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_recon_loss = 0.0\n","        running_kl_loss = 0.0\n","\n","        for _, (data, library) in enumerate(train_loader):\n","            data = data.to(dev)\n","            library = library.to(dev)\n","            opt.zero_grad()\n","            x_hat, mu, log_var = model(data)\n","            x_hat = F.softmax(x_hat, dim=-1) * library.unsqueeze(-1)\n","            recon_loss, kl_div = model.loss(data, x_hat, mu, log_var)\n","            loss = recon_loss + kl_weight * kl_div\n","            loss.backward()\n","            opt.step()\n","\n","            running_loss += loss.item()\n","            running_recon_loss += recon_loss.item()\n","            running_kl_loss += kl_div.item()\n","\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_recon_loss = running_recon_loss / len(train_loader.dataset)\n","        epoch_kl_loss = running_kl_loss / len(train_loader.dataset)\n","        loss_history['train_loss'].append(epoch_loss)\n","        loss_history['recon_loss'].append(epoch_recon_loss)\n","        loss_history['kl_loss'].append(epoch_kl_loss)\n","        print(f\"Epoch: {epoch}, Loss: {epoch_loss}, Recon Loss: {epoch_recon_loss}, KL Loss: {epoch_kl_loss}\")\n","    \n","    return loss_history\n","\n","vae_loss_hist = train_vae(model, train_loader, 100)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1676637865926,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"2bNv2K-HRkTW","outputId":"bfeabde4-31f0-41e8-f11d-e290af037429"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reconstruction loss: 1.3369\n"]}],"source":["# Calculate reconstruction loss on \"test\" subset\n","model.eval()\n","test, test_lib = next(iter(DataLoader(train_dataset, batch_size=200, shuffle=False)))\n","test, test_lib = test.to(dev), test_lib.to(dev)\n","with torch.no_grad():\n","    x_hat, mu, log_var,= model(test)\n","    x_hat = F.softmax(x_hat, dim=-1) * test_lib.unsqueeze(-1)\n","    loss = F.mse_loss(x_hat, test, reduction='mean')\n","    print(f\"Reconstruction loss: {loss.item():.4f}\")"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":551,"status":"ok","timestamp":1676637866467,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"dYRfZqCIUZG_","outputId":"9dbe7bce-ef74-4189-b3e3-17716c0e302f"},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","GMVAE                                    [4096, 2766]              --\n","├─Sequential: 1-1                        [4096, 20]                --\n","│    └─Linear: 2-1                       [4096, 512]               1,416,704\n","│    └─ReLU: 2-2                         [4096, 512]               --\n","│    └─Linear: 2-3                       [4096, 256]               131,328\n","│    └─ReLU: 2-4                         [4096, 256]               --\n","│    └─Linear: 2-5                       [4096, 20]                5,140\n","├─Sequential: 1-2                        [4096, 2766]              --\n","│    └─Linear: 2-6                       [4096, 256]               768\n","│    └─ReLU: 2-7                         [4096, 256]               --\n","│    └─Linear: 2-8                       [4096, 512]               131,584\n","│    └─ReLU: 2-9                         [4096, 512]               --\n","│    └─Linear: 2-10                      [4096, 2766]              1,418,958\n","==========================================================================================\n","Total params: 3,104,482\n","Trainable params: 3,104,482\n","Non-trainable params: 0\n","Total mult-adds (G): 12.72\n","==========================================================================================\n","Input size (MB): 45.32\n","Forward/backward pass size (MB): 141.62\n","Params size (MB): 12.42\n","Estimated Total Size (MB): 199.36\n","=========================================================================================="]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# ------------------------\n","# - Gaussian Mixture VAE -\n","# ------------------------\n","from torchinfo import summary\n","\n","class GMVAE(nn.Module):\n","    def __init__(self, input_dim, latent_dim, num_clusters):\n","        super(GMVAE, self).__init__()\n","        self.input_dim = input_dim\n","        self.latent_dim = latent_dim\n","        self.K = num_clusters\n","        \n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, latent_dim*num_clusters*2)\n","        )\n","        \n","        self.decoder = self.decode = nn.Sequential(\n","            nn.Linear(latent_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, input_dim)\n","        )\n","\n","    def encode(self, x):\n","        h = self.encoder(x)\n","        mu, log_var = torch.chunk(h, 2, dim=-1)\n","        mu = mu.view(-1, self.K, self.latent_dim)\n","        log_var = log_var.view(-1, self.K, self.latent_dim)\n","        return mu, log_var\n","\n","    def reparameterize(self, mu, log_var, temperature=1.0):\n","        eps = torch.randn_like(log_var)\n","        z = mu + torch.exp(log_var / 2) * eps\n","        # Gumbel-Softmax trick \n","        gumbel_softmax_logits = (z.view(-1, self.K, self.latent_dim) / temperature).softmax(dim=-1) # (B, K, latent_dim)\n","        z = (gumbel_softmax_logits * z.view(-1, self.K, self.latent_dim)).sum(dim=1) # (B, latent_dim)\n","        return z, gumbel_softmax_logits\n","\n","\n","    def forward(self, x, temperature=1.0):\n","        x = x.view(-1, self.input_dim)\n","        mu, log_var = self.encode(x)\n","        z, gumbel_softmax_logits = self.reparameterize(mu, log_var, temperature=temperature)\n","        x_hat = self.decode(z)\n","        return x_hat, mu, log_var, z, gumbel_softmax_logits\n","\n","dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","gm = GMVAE(input_dim=next(iter(train_loader))[0].shape[-1], latent_dim=2, num_clusters=5).to(dev)\n","gm.train()\n","summary(gm, input_size=(2**12, 2766), device=dev)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1328,"status":"ok","timestamp":1676638463546,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"0gOcwJH2awX3","outputId":"5835c3e2-d317-455b-c35d-f10f9838939b"},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","VAE                                      [4096, 2766]              --\n","├─Sequential: 1-1                        [4096, 4]                 --\n","│    └─Linear: 2-1                       [4096, 512]               1,416,704\n","│    └─ReLU: 2-2                         [4096, 512]               --\n","│    └─Linear: 2-3                       [4096, 256]               131,328\n","│    └─ReLU: 2-4                         [4096, 256]               --\n","│    └─Linear: 2-5                       [4096, 4]                 1,028\n","├─Sequential: 1-2                        [4096, 2766]              --\n","│    └─Linear: 2-6                       [4096, 256]               768\n","│    └─ReLU: 2-7                         [4096, 256]               --\n","│    └─Linear: 2-8                       [4096, 512]               131,584\n","│    └─ReLU: 2-9                         [4096, 512]               --\n","│    └─Linear: 2-10                      [4096, 2766]              1,418,958\n","==========================================================================================\n","Total params: 3,100,370\n","Trainable params: 3,100,370\n","Non-trainable params: 0\n","Total mult-adds (G): 12.70\n","==========================================================================================\n","Input size (MB): 45.32\n","Forward/backward pass size (MB): 141.10\n","Params size (MB): 12.40\n","Estimated Total Size (MB): 198.82\n","=========================================================================================="]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["summary(model, input_size=(2**12, 2766), device=dev)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292832,"status":"ok","timestamp":1676638159293,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"ESIx0tgqUbVY","outputId":"d02313da-eff0-4688-ea18-3cd7e99f735c"},"outputs":[{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100 - Loss: 58083.6649 - Recon loss: 58083.4418 - KL loss: 0.2245\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 2/100 - Loss: 17235.2857 - Recon loss: 17235.2653 - KL loss: 0.0206\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 3/100 - Loss: 14364.0486 - Recon loss: 14364.0097 - KL loss: 0.0393\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 4/100 - Loss: 11861.1155 - Recon loss: 11861.0566 - KL loss: 0.0598\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 5/100 - Loss: 10796.2282 - Recon loss: 10796.1594 - KL loss: 0.0702\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 6/100 - Loss: 10857.0230 - Recon loss: 10856.9541 - KL loss: 0.0709\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 7/100 - Loss: 10897.6239 - Recon loss: 10897.5619 - KL loss: 0.0640\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 8/100 - Loss: 10434.5676 - Recon loss: 10434.5098 - KL loss: 0.0600\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 9/100 - Loss: 10233.4475 - Recon loss: 10233.3892 - KL loss: 0.0607\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 10/100 - Loss: 9952.3929 - Recon loss: 9952.3268 - KL loss: 0.0691\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 11/100 - Loss: 9698.4284 - Recon loss: 9698.3540 - KL loss: 0.0784\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 12/100 - Loss: 9556.2228 - Recon loss: 9556.1413 - KL loss: 0.0863\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 13/100 - Loss: 9366.9952 - Recon loss: 9366.9124 - KL loss: 0.0881\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 14/100 - Loss: 9150.9786 - Recon loss: 9150.8876 - KL loss: 0.0974\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 15/100 - Loss: 8958.4506 - Recon loss: 8958.3551 - KL loss: 0.1027\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 16/100 - Loss: 8819.1051 - Recon loss: 8819.0025 - KL loss: 0.1109\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 17/100 - Loss: 8792.3428 - Recon loss: 8792.2363 - KL loss: 0.1160\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 18/100 - Loss: 8648.4724 - Recon loss: 8648.3616 - KL loss: 0.1211\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 19/100 - Loss: 8561.6127 - Recon loss: 8561.5030 - KL loss: 0.1206\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 20/100 - Loss: 8224.9711 - Recon loss: 8224.8629 - KL loss: 0.1196\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 21/100 - Loss: 8043.7611 - Recon loss: 8043.6353 - KL loss: 0.1399\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 22/100 - Loss: 7777.5197 - Recon loss: 7777.4013 - KL loss: 0.1324\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 23/100 - Loss: 7599.0128 - Recon loss: 7598.8858 - KL loss: 0.1430\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 24/100 - Loss: 7486.2427 - Recon loss: 7486.1178 - KL loss: 0.1413\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 25/100 - Loss: 7370.8004 - Recon loss: 7370.6686 - KL loss: 0.1499\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 26/100 - Loss: 7501.6477 - Recon loss: 7501.5211 - KL loss: 0.1450\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 27/100 - Loss: 7343.0708 - Recon loss: 7342.9499 - KL loss: 0.1392\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 28/100 - Loss: 7471.4352 - Recon loss: 7471.3138 - KL loss: 0.1406\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 29/100 - Loss: 7421.7799 - Recon loss: 7421.6680 - KL loss: 0.1304\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 30/100 - Loss: 7074.5400 - Recon loss: 7074.4246 - KL loss: 0.1352\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 31/100 - Loss: 6969.7158 - Recon loss: 6969.5934 - KL loss: 0.1443\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 32/100 - Loss: 6896.4606 - Recon loss: 6896.3370 - KL loss: 0.1466\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 33/100 - Loss: 6833.5505 - Recon loss: 6833.4193 - KL loss: 0.1565\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 34/100 - Loss: 6723.0671 - Recon loss: 6722.9362 - KL loss: 0.1571\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 35/100 - Loss: 6670.0920 - Recon loss: 6669.9577 - KL loss: 0.1621\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 36/100 - Loss: 6637.3813 - Recon loss: 6637.2442 - KL loss: 0.1664\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 37/100 - Loss: 6502.0405 - Recon loss: 6501.8988 - KL loss: 0.1732\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 38/100 - Loss: 6414.0204 - Recon loss: 6413.8739 - KL loss: 0.1802\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 39/100 - Loss: 6309.7494 - Recon loss: 6309.5988 - KL loss: 0.1864\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 40/100 - Loss: 6263.3807 - Recon loss: 6263.2274 - KL loss: 0.1908\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 41/100 - Loss: 6164.6666 - Recon loss: 6164.5115 - KL loss: 0.1944\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 42/100 - Loss: 6113.9577 - Recon loss: 6113.7974 - KL loss: 0.2021\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 43/100 - Loss: 6099.2703 - Recon loss: 6099.1091 - KL loss: 0.2046\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 44/100 - Loss: 6002.8290 - Recon loss: 6002.6663 - KL loss: 0.2078\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 45/100 - Loss: 6018.6656 - Recon loss: 6018.4970 - KL loss: 0.2168\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 46/100 - Loss: 5969.9801 - Recon loss: 5969.8199 - KL loss: 0.2073\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 47/100 - Loss: 5890.8125 - Recon loss: 5890.6496 - KL loss: 0.2121\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 48/100 - Loss: 5796.3524 - Recon loss: 5796.1866 - KL loss: 0.2175\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 49/100 - Loss: 5715.3092 - Recon loss: 5715.1415 - KL loss: 0.2215\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 50/100 - Loss: 5718.4554 - Recon loss: 5718.2858 - KL loss: 0.2253\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 51/100 - Loss: 5653.0453 - Recon loss: 5652.8720 - KL loss: 0.2317\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 52/100 - Loss: 5828.7379 - Recon loss: 5828.5710 - KL loss: 0.2250\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 53/100 - Loss: 5757.2941 - Recon loss: 5757.1344 - KL loss: 0.2166\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 54/100 - Loss: 5660.7189 - Recon loss: 5660.5477 - KL loss: 0.2338\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 55/100 - Loss: 5668.7303 - Recon loss: 5668.5755 - KL loss: 0.2130\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 56/100 - Loss: 5587.5778 - Recon loss: 5587.4213 - KL loss: 0.2167\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 57/100 - Loss: 5533.4998 - Recon loss: 5533.3473 - KL loss: 0.2126\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 58/100 - Loss: 5610.1337 - Recon loss: 5609.9809 - KL loss: 0.2146\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 59/100 - Loss: 5528.7272 - Recon loss: 5528.5699 - KL loss: 0.2224\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 60/100 - Loss: 5456.0181 - Recon loss: 5455.8679 - KL loss: 0.2139\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 61/100 - Loss: 5464.6947 - Recon loss: 5464.5405 - KL loss: 0.2213\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 62/100 - Loss: 5445.9529 - Recon loss: 5445.8050 - KL loss: 0.2138\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 63/100 - Loss: 5399.4630 - Recon loss: 5399.3110 - KL loss: 0.2214\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 64/100 - Loss: 5435.4464 - Recon loss: 5435.3022 - KL loss: 0.2114\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 65/100 - Loss: 5398.6974 - Recon loss: 5398.5580 - KL loss: 0.2059\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 66/100 - Loss: 5318.8223 - Recon loss: 5318.6814 - KL loss: 0.2096\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 67/100 - Loss: 5318.1381 - Recon loss: 5317.9973 - KL loss: 0.2112\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 68/100 - Loss: 5323.4246 - Recon loss: 5323.2831 - KL loss: 0.2139\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 69/100 - Loss: 5310.1450 - Recon loss: 5310.0052 - KL loss: 0.2129\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 70/100 - Loss: 5322.2750 - Recon loss: 5322.1383 - KL loss: 0.2098\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 71/100 - Loss: 5263.7263 - Recon loss: 5263.5880 - KL loss: 0.2140\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 72/100 - Loss: 5210.2702 - Recon loss: 5210.1313 - KL loss: 0.2165\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 73/100 - Loss: 5182.8872 - Recon loss: 5182.7369 - KL loss: 0.2362\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 74/100 - Loss: 5168.3975 - Recon loss: 5168.2585 - KL loss: 0.2202\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 75/100 - Loss: 5139.8783 - Recon loss: 5139.7309 - KL loss: 0.2354\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 76/100 - Loss: 5136.3567 - Recon loss: 5136.2161 - KL loss: 0.2265\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 77/100 - Loss: 5120.4386 - Recon loss: 5120.2992 - KL loss: 0.2262\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 78/100 - Loss: 5132.4934 - Recon loss: 5132.3490 - KL loss: 0.2362\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 79/100 - Loss: 5171.9089 - Recon loss: 5171.6542 - KL loss: 0.4202\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 80/100 - Loss: 5163.7481 - Recon loss: 5163.6171 - KL loss: 0.2179\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 81/100 - Loss: 5138.1152 - Recon loss: 5137.9824 - KL loss: 0.2227\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 82/100 - Loss: 5196.0682 - Recon loss: 5195.9276 - KL loss: 0.2381\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 83/100 - Loss: 5073.0510 - Recon loss: 5072.9237 - KL loss: 0.2171\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 84/100 - Loss: 5025.9417 - Recon loss: 5025.8158 - KL loss: 0.2166\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 85/100 - Loss: 4997.1667 - Recon loss: 4997.0395 - KL loss: 0.2211\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 86/100 - Loss: 4978.2216 - Recon loss: 4978.0893 - KL loss: 0.2317\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 87/100 - Loss: 4992.4864 - Recon loss: 4992.3572 - KL loss: 0.2284\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 88/100 - Loss: 4989.5115 - Recon loss: 4989.3130 - KL loss: 0.3542\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 89/100 - Loss: 4962.2419 - Recon loss: 4962.1153 - KL loss: 0.2279\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 90/100 - Loss: 4940.7956 - Recon loss: 4940.6344 - KL loss: 0.2929\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 91/100 - Loss: 4947.9759 - Recon loss: 4947.8517 - KL loss: 0.2277\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 92/100 - Loss: 4926.9801 - Recon loss: 4926.8550 - KL loss: 0.2315\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 93/100 - Loss: 4928.4695 - Recon loss: 4928.3461 - KL loss: 0.2305\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 94/100 - Loss: 4909.8979 - Recon loss: 4909.7759 - KL loss: 0.2299\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 95/100 - Loss: 4951.0821 - Recon loss: 4950.9617 - KL loss: 0.2293\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 96/100 - Loss: 4874.0830 - Recon loss: 4873.9567 - KL loss: 0.2428\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 97/100 - Loss: 4811.9611 - Recon loss: 4811.8394 - KL loss: 0.2363\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 98/100 - Loss: 4807.7735 - Recon loss: 4807.6526 - KL loss: 0.2370\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 99/100 - Loss: 4797.3958 - Recon loss: 4797.2719 - KL loss: 0.2453\n"]},{"name":"stderr","output_type":"stream","text":["                                                                 "]},{"name":"stdout","output_type":"stream","text":["Epoch 100/100 - Loss: 4812.8915 - Recon loss: 4812.7689 - KL loss: 0.2450\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["import torch.optim as optim\n","from tqdm import tqdm\n","\n","def gmvae_loss(x, x_hat, mu, log_var, z, gumbel_softmax_logits, temperature=1.0):\n","    # Reconstruction loss\n","    recon_loss = F.mse_loss(x_hat, x, reduction='sum')\n","\n","    # KL divergence loss\n","    kl_div = (torch.exp(log_var) + mu**2 - 1 - log_var).sum(dim=-1)\n","    kl_loss = torch.mean(torch.sum(gumbel_softmax_logits * kl_div.unsqueeze(-1), dim=-1))\n","    \n","    # Total loss\n","    loss = recon_loss + temperature * kl_loss\n","    \n","    return loss, recon_loss, kl_loss\n","\n","\n","def train(model, dataloader, num_epochs, lr=1e-3, weight_decay=1e-5, initial_temperature=1.0, final_temperature=0.5, device='cpu'): \n","    model.to(device)\n","    model.train()\n","\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    loss_history = {'train_loss': [], 'recon_loss': [], 'kl_loss': []}\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_recon_loss = 0.0\n","        running_kl_loss = 0.0\n","\n","        # Compute the temperature for this epoch based on the linear annealing schedule\n","        temperature = max(initial_temperature - epoch * (initial_temperature - final_temperature) / (num_epochs - 1), final_temperature)\n","        for x, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\", leave=False):\n","            x = x.to(device)\n","            optimizer.zero_grad()\n","            x_hat, mu, log_var, z, gumbel_softmax_logits = model(x, temperature=temperature)\n","            loss, recon_loss, kl_loss = gmvae_loss(x, x_hat, mu, log_var, z, gumbel_softmax_logits, temperature=temperature)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","            running_recon_loss += recon_loss.item()\n","            running_kl_loss += kl_loss.item()\n","\n","        epoch_loss = running_loss / len(dataloader.dataset)\n","        epoch_recon_loss = running_recon_loss / len(dataloader.dataset)\n","        epoch_kl_loss = running_kl_loss / len(dataloader.dataset)\n","\n","        loss_history['train_loss'].append(epoch_loss)\n","        loss_history['recon_loss'].append(epoch_recon_loss)\n","        loss_history['kl_loss'].append(epoch_kl_loss)\n","        tqdm.write(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Recon loss: {epoch_recon_loss:.4f} - KL loss: {epoch_kl_loss:.4f}\")\n","    model.eval()\n","    \n","    return loss_history\n","\n","gm_loss_hist = train(gm, train_loader, num_epochs=100, initial_temperature=1.0, final_temperature=0.5, lr=1e-3, weight_decay=1e-5, device=dev)\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1676638159294,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"j-VNXh9jUivq","outputId":"238d37d9-2b8e-41fb-d99d-4f3103c5d743"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reconstruction loss: 1.4161\n"]}],"source":["# Calculate reconstruction loss on the complete dataset\n","model.eval()\n","\n","with torch.no_grad():\n","    x_hat, mu, log_var, z, gumbel_softmax_logits = gm(test, temperature=0.5)\n","    loss = F.mse_loss(x_hat, test, reduction='mean')\n","    print(f\"Reconstruction loss: {loss.item():.4f}\")"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1676638160224,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"CcVnHkBoXnGa","outputId":"5e3c5d85-c31c-4e11-dedf-8463544bc3d2"},"outputs":[{"data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7f9e82251af0>"]},"execution_count":25,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["from matplotlib import pyplot as plt\n","\n","zt = z.cpu().detach().numpy()\n","\n","plt.scatter(zt[:,0], zt[:,1])"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":243},"executionInfo":{"elapsed":9,"status":"error","timestamp":1676638160224,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"qXA4Bf7FUjxd","outputId":"402168da-b880-4a42-fa69-3e380d529efb"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-a1e6fcd048d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdsa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrecon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recon_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'asdsa' is not defined"]}],"source":["asdsa\n","import time\n","import pickle\n","\n","recon_loss = loss_history['recon_loss'][-1]\n","timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n","model_path = f\"models/gmvae_{timestamp}_{recon_loss}.pt\"\n","loss_history_path = f\"loss_history/gmvae_{timestamp}_{recon_loss}.pkl\"\n","\n","torch.save(gm.state_dict(), gm)\n","with open(loss_history_path, 'wb') as f:\n","    pickle.dump(loss_history, f)\n","\n","## LOAD MODEL AND LOSS HISTORY\n","# model_path = \"models/gmvae_20210104-174210.pt\"\n","# loss_history_path = \"loss_history/gmvae_20210104-174210.pkl\"\n","\n","# model.load_state_dict(torch.load(model_path))\n","# with open(loss_history_path, 'rb') as f:\n","#     loss_history = pickle.load(f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1676638160225,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"mbm0g5fKUo0T"},"outputs":[],"source":["# z = model.reparameterize(*model.encoder(test))\n","\n","# for i in range(z.shape[1]):\n","#     model.decoder.zero_grad()\n","#     leaf = z[0].detach().requires_grad_()\n","#     out = model.decoder(z[0])[i]\n","#     out.retain_grad()\n","#     out.backward(retain_graph=True)\n","#     gene_gradients.append(out.grad.clone().detach())\n","\n","\n","# G = torch.stack(gene_gradients)\n","\n","# G"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1676638160225,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"B_FwvDkfUpaO"},"outputs":[],"source":["from functorch import jacfwd, vmap\n","\n","def jac(f, z):\n","    # composed with vmap for batched Jacobians\n","    return vmap(jacfwd(f))(z)\n","    \n","def jac_robust(f, z):\n","    # alternative jac if experiencing crashes \n","    batch_size, z_dim = z.size()\n","    v = torch.eye(z_dim).unsqueeze(0).repeat(batch_size, 1, 1).view(-1, z_dim).to(z)\n","    z = z.repeat(1, z_dim).view(-1, z_dim)\n","    return torch.autograd.functional.jvp(f, z, v=v)[1].view(batch_size, z_dim, -1).permute(0, 2, 1)\n","\n","# gene \"correlation\" matrix C\n","def C_matrix(f, z, normalize=False):\n","    '''\n","    f:      model.decode: (b, m) -> (b, n) \n","    z:      torch.tensor whose size = (b, m) (keep b low for memory)\n","    out:    torch.tensor whose size = (b, n, n)\n","    '''\n","    J = jac(f, z)\n","\n","    # J @ J.T on batch \n","    if normalize:\n","        return torch.bmm(J, J.transpose(1,2)) / J.norm(p=2, dim=2).unsqueeze(-1)\n","    else:\n","        return torch.bmm(J, J.transpose(1,2))\n","\n","# jacrev: 1.45s\n","# jacfwd: 80-90ms \n","# jvp:    85-92ms "]},{"cell_type":"markdown","metadata":{"id":"bbWh1dYGdaST"},"source":["## Trajectories"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1676638160225,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"AWImcXz03udq"},"outputs":[],"source":["def get_Riemannian_metric(f, z, create_graph=False): #J.T @ J instead! \n","    J = jac_robust(f, z)\n","    out = torch.einsum('nij,nik->njk', J, J)\n","    return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1676638160226,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"yWsB3ScDdbZM"},"outputs":[],"source":["def compute_geodesic(z1, z2, pretrained_model, get_Riemannian_metric, num_discretization=100):\n","    '''\n","    z1 : torch.tensor whose size = (1, 2)\n","    z1 : torch.tensor whose size = (1, 2)\n","    out: torch.tensor whose size = (num_discretization, 2)\n","    '''\n","    from scipy.optimize import minimize\n","    class GeodesicFittingTool():\n","        def __init__(self, z1, z2, z_init, pretrained_model, get_Riemannian_metric, num_discretization, method, device=f\"cuda:{0}\"):\n","            self.z1 = z1\n","            self.z2 = z2\n","            self.pretrained_model = pretrained_model\n","            self.get_Riemannian_metric = get_Riemannian_metric\n","            self.num_discretization = num_discretization\n","            self.delta_t = 1/(num_discretization-1)\n","            self.device = device\n","            self.method = method\n","            self.z_init_input = z_init\n","            self.initialize()\n","            \n","        def initialize(self):\n","            self.z_init= self.z1.squeeze(0)\n","            self.z_final= self.z2.squeeze(0)\n","            dim = self.z_final.size(0)\n","            self.init_z = self.z_init_input.detach().cpu().numpy()\n","            self.z_shape = self.init_z.shape\n","            self.init_z_vec = self.init_z.flatten()\n","\n","        def geodesic_loss(self, z): \n","            z_torch = torch.tensor(z.reshape(self.z_shape), dtype=torch.float32).to(self.device)\n","            z_extended = torch.cat([self.z_init.unsqueeze(0), z_torch, self.z_final.unsqueeze(0)], dim=0)\n","            G_ = self.get_Riemannian_metric(self.pretrained_model.decode, z_extended[:-1])\n","            delta_z = (z_extended[1:, :]-z_extended[:-1, :])/(self.delta_t)\n","            loss = torch.einsum('ni, nij, nj -> ', delta_z, G_, delta_z) * self.delta_t\n","            return loss.item()\n","        \n","        def jac(self, z):\n","            z_torch = torch.tensor(z.reshape(self.z_shape), dtype=torch.float32).to(self.device)\n","            z_torch.requires_grad = True\n","            z_extended = torch.cat([self.z_init.unsqueeze(0), z_torch, self.z_final.unsqueeze(0)], dim=0)\n","            G_ = self.get_Riemannian_metric(self.pretrained_model.decode, z_extended[:-1], create_graph=True)\n","            delta_z = (z_extended[1:, :]-z_extended[:-1, :])/(self.delta_t)\n","            loss = torch.einsum('ni, nij, nj -> ', delta_z, G_, delta_z) * self.delta_t\n","            loss.backward()\n","            z_grad = z_torch.grad\n","            return z_grad.detach().cpu().numpy().flatten()\n","\n","        def callback(self, z):\n","            self.Nfeval += 1\n","            return print('{} th loss : {}'.format(self.Nfeval, self.geodesic_loss(z)))\n","            \n","        def BFGS_optimizer(self, callback=False, maxiter=1000):\n","            self.Nfeval = 0\n","            z0 = self.init_z_vec\n","            if callback == True:\n","                call = self.callback\n","            else:\n","                call = None\n","            res = minimize(\n","                self.geodesic_loss, \n","                z0, \n","                callback=call, \n","                method=self.method,\n","                jac = self.jac,\n","                options = {\n","                    'gtol': 1e-10, \n","                    'eps': 1.4901161193847656e-08, \n","                    'maxiter': maxiter, \n","                    'disp': True, \n","                    'return_all': False, \n","                    'finite_diff_rel_step': None}\n","                )\n","            self.res = res\n","\n","    z12_linear_curve = torch.cat([z1.to(device) + (z2.to(device) - z1.to(device)) * t/(num_discretization-1) for t in range(num_discretization)], dim=0)\n","    \n","    tool = GeodesicFittingTool(z1, z2, z12_linear_curve[1:-1], pretrained_model, get_Riemannian_metric, num_discretization, 'BFGS', device=device)\n","    tool.BFGS_optimizer()\n","    z_torch = torch.tensor(tool.res['x'].reshape(tool.z_shape), dtype=torch.float32).to(device)\n","    out = torch.cat([tool.z_init.unsqueeze(0), z_torch, tool.z_final.unsqueeze(0)], dim=0)\n","    return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1676638160226,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"3TbwaJy63ktw"},"outputs":[],"source":["z1 = torch.tensor([[-1, 1]], dtype=torch.float32) \n","z2 = torch.tensor([[0.6, -1.2]], dtype=torch.float32) \n","\n","num_discretization = 100\n","z12_geodesic_curve = compute_geodesic(\n","    z1.to(device), \n","    z2.to(device), \n","    pretrained_model, \n","    get_Riemannian_metric, \n","    num_discretization=num_discretization\n",")\n","assert z12_geodesic_curve.size() == torch.Size([num_discretization, 2])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1676638160226,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"IOkivSru5FLc"},"outputs":[],"source":["L12 = compute_length_of_curve(\n","    z12_geodesic_curve, pretrained_model, get_Riemannian_metric)\n","L12\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1676638160226,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"XQ3qhCoe5zF4"},"outputs":[],"source":["z12_linear_curve = torch.cat([z1.to(device) + (z2.to(device) - z1.to(device)) * t/(num_discretization-1) for t in range(num_discretization)], dim=0)\n","\n","latent_embeddings = pretrained_model.encode(\n","    train_ds.data.to(device)\n","    ).detach().cpu().numpy()\n","\n","z_scale = np.minimum(\n","    np.max(latent_embeddings, axis=0), \n","    np.min(latent_embeddings, axis=0)\n",")\n","labels = torch.unique(train_ds.targets)\n","\n","f = plt.figure(figsize=(7, 7))\n","plt.title('Latent space embeddings')\n","for label in labels:\n","    classwise_le = latent_embeddings[train_ds.targets == label]\n","    plt.scatter(\n","        classwise_le[:200, 0], \n","        classwise_le[:200, 1], \n","        label=label.item(), s=5)\n","\n","plt.scatter(z1[0, 0], z1[0, 1], c='tab:red', marker='s')\n","plt.scatter(z2[0, 0], z2[0, 1], c='tab:green', marker='s')\n","\n","plt.scatter(z12_linear_curve[:, 0].detach().cpu(), z12_linear_curve[:, 1].detach().cpu(), s=10, c='tab:gray')\n","plt.scatter(z12_geodesic_curve[:, 0].detach().cpu(), z12_geodesic_curve[:, 1].detach().cpu(), s=10, c='tab:pink')\n","\n","plt.xlim(-3, 3)\n","plt.ylim(-3, 3)\n","\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1676638160227,"user":{"displayName":"Andreas Jeppesen","userId":"00540306493284899945"},"user_tz":-60},"id":"eJOoNKJr5zrN"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMpRtQxFNFqh1SEVkdndz4h","mount_file_id":"1gEzMrTlr7G_N89-yMbnXE_sKWSHFhCj9","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
